{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TensorFlow 101</center>\n",
    "<center> Shan-Hung Wu & DataLab </center>\n",
    "<center> Fall 2025 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications. TensorFlow has several benefits:\n",
    "1. Easy model building\n",
    "2. Robust ML production anywhere\n",
    "3. Powerful experimentation for research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use TensorFlow as our framework in the following lectures. In this lab, you will learn how to install TensorFlow and get a better understanding by implementing a classical deep learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software requirements\n",
    "\n",
    "The following NVIDIA® software must be installed on your system:\n",
    "\n",
    "- NVIDIA® GPU drivers —CUDA® 10.1 requires 418.x or higher.\n",
    "- CUDA® Toolkit —TensorFlow supports CUDA® 10.1 (TensorFlow >= 2.1.0).\n",
    "- CUPTI ships with the CUDA® Toolkit.\n",
    "- cuDNN SDK 7.6 (see cuDNN versions).\n",
    "- (Optional) TensorRT 6.0 to improve latency and throughput for inference on some models.\n",
    "\n",
    "Please refer to TensorFlow website, [GPU Support](https://www.tensorflow.org/install/gpu) section, for more details and latest information. Please check the version of the abovementioned softwares carefully. There is a strict requirement between TensorFlow's version and NVIDIA® softwares'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install CUDA with apt\n",
    "This section shows how to install CUDA® 10 (TensorFlow >= 1.13.0) on Ubuntu 16.04 and 18.04. These instructions may work for other Debian-based distros.\n",
    "\n",
    "### Ubuntu 18.04 (CUDA 10.1)\n",
    "```bash\n",
    "# Add NVIDIA package repositories\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\n",
    "sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
    "sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\n",
    "sudo apt-get update\n",
    "wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "sudo apt-get update\n",
    "\n",
    "# Install NVIDIA driver\n",
    "sudo apt-get install --no-install-recommends nvidia-driver-450\n",
    "# Reboot. Check that GPUs are visible using the command: nvidia-smi\n",
    "\n",
    "# Install development and runtime libraries (~4GB)\n",
    "sudo apt-get install --no-install-recommends \\\n",
    "    cuda-10-1 \\\n",
    "    libcudnn7=7.6.5.32-1+cuda10.1  \\\n",
    "    libcudnn7-dev=7.6.5.32-1+cuda10.1\n",
    "\n",
    "\n",
    "# Install TensorRT. Requires that libcudnn7 is installed above.\n",
    "sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\n",
    "    libnvinfer-dev=6.0.1-1+cuda10.1 \\\n",
    "    libnvinfer-plugin6=6.0.1-1+cuda10.1\n",
    "```\n",
    "\n",
    "### Ubuntu 16.04 (CUDA 10.1)\n",
    "```bash\n",
    "# Add NVIDIA package repositories\n",
    "# Add HTTPS support for apt-key\n",
    "sudo apt-get install gnupg-curl\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.1.243-1_amd64.deb\n",
    "sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n",
    "sudo dpkg -i cuda-repo-ubuntu1604_10.1.243-1_amd64.deb\n",
    "sudo apt-get update\n",
    "wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\n",
    "sudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\n",
    "sudo apt-get update\n",
    "\n",
    "# Install NVIDIA driver\n",
    "# Issue with driver install requires creating /usr/lib/nvidia\n",
    "sudo mkdir /usr/lib/nvidia\n",
    "sudo apt-get install --no-install-recommends nvidia-418\n",
    "# Reboot. Check that GPUs are visible using the command: nvidia-smi\n",
    "\n",
    "# Install development and runtime libraries (~4GB)\n",
    "sudo apt-get install --no-install-recommends \\\n",
    "    cuda-10-1 \\\n",
    "    libcudnn7=7.6.4.38-1+cuda10.1  \\\n",
    "    libcudnn7-dev=7.6.4.38-1+cuda10.1\n",
    "\n",
    "\n",
    "# Install TensorRT. Requires that libcudnn7 is installed above.\n",
    "sudo apt-get install -y --no-install-recommends \\\n",
    "    libnvinfer6=6.0.1-1+cuda10.1 \\\n",
    "    libnvinfer-dev=6.0.1-1+cuda10.1 \\\n",
    "    libnvinfer-plugin6=6.0.1-1+cuda10.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install CUDA with Anaconda\n",
    "\n",
    "You can also install CUDA with Anaconda with following command:\n",
    "\n",
    "```bash\n",
    "conda install cudnn=7.6.5=cuda10.1_0\n",
    "```\n",
    "\n",
    ",which will install CUDA Toolkit and cuDNN SDK. After that, the only thing you have to install manually is Nvidia driver.\n",
    "After installing CUDA Toolkit, you can check CUDA version with following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\n",
      "Cuda compilation tools, release 11.2, V11.2.152\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check GPU utilization after installing GPU driver with following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 11 15:39:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:19:00.0 Off |                  N/A |\n",
      "|  0%   32C    P8    19W / 260W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8     4W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8     4W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:68:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    26W / 250W |      0MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is tested and supported on the following 64-bit systems:\n",
    "- Python 3.5–3.8\n",
    "- Ubuntu 16.04 or later\n",
    "- macOS 10.12.6 (Sierra) or later (no GPU support)\n",
    "- Windows 7 or later \n",
    "- Raspbian 9.0 or later\n",
    "\n",
    "We can simply install TensorFlow with Python's `pip` package manager with following commands:\n",
    "\n",
    "```bash\n",
    "# Requires the latest pip\n",
    "pip install --upgrade pip\n",
    "\n",
    "# Current stable release for CPU and GPU\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommanded to install TensorFlow in a virtual environment, for more details, please refer to [Install TensorFlow with pip](https://www.tensorflow.org/install/pip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether TensorFlow is installed successfully and confirm that TensorFlow is using the GPU by executing following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.9.1\n",
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab\n",
    "Installing all dependencies of TensorFlow is a pain in the ass. Google Colab provides a Jupyter notebook environment that requires no setup with free GPU. The types of GPUs that are available in Colab vary over time. The GPUs available in Colab often include Nvidia\n",
    "- K80\n",
    "- T4\n",
    "- P4\n",
    "- P100\n",
    "\n",
    "There is no way to choose what type of GPU you can connect to in Colab at any given time.\n",
    "\n",
    "You can easily access Google Colab via your Google account and play around with TensorFlow. For example, [TensorFlow 2 quickstart for beginners](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb).\n",
    "\n",
    "However, there are few constraints when using Google Colab:\n",
    "- 12 hours lifetimes limit\n",
    "- Various available GPU memory\n",
    "\n",
    "Google announced a new service called [Colab Pro](https://colab.research.google.com/signup) ($9.99/month) at the beginning of 2020, which provides faster GPUs, longer runtimes, and more memory compared with Colab. Other choices for acquiring GPU is to use [Cloud GPU](https://cloud.google.com/gpu) and [Cloud TPU](https://cloud.google.com/tpu), but they are more expensive. As a result, we highly recommend you to use TensorFlow on your own machine, while colab is still a good place for you to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 quickstart\n",
    "Originally developed by Google Brain, TensorFlow is an open source library which provides a variety of functions and classes used to conduct machine learning algorithm.\n",
    "\n",
    "TensorFlow has various benefits, including\n",
    "- Python API\n",
    "- Portability: can be distributed on multiple CPUs, GPUs, or TPUs as well as on mobile devices\n",
    "- Flexibility: can run on different devices e.g. Raspberry Pi, Android, iOS, Windows, Linux\n",
    "- Visualization: visualize the training process via TensorBoard\n",
    "- Checkpoints: manage trained models\n",
    "- Auto-differentiation\n",
    "- Large community\n",
    "\n",
    "Later on, we will try to:\n",
    "- Load dataset via `tf.keras.datasets`\n",
    "- Build model via high-level *Sequential API*\n",
    "- Build model via more flexible *Functional API*\n",
    "- Build model via fully-customizable *Model Subclassing*\n",
    "- Boost performance by `tf.function`\n",
    "- Customize gradient flow by `tf.custom_gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit GPU memory growth\n",
    "By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to `CUDA_VISIBLE_DEVICES`) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the `tf.config.experimental.set_visible_devices` method.\n",
    "\n",
    "In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. One option is to turn on memory growth by calling `tf.config.experimental.set_memory_growth`, which attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, we extend the GPU memory region allocated to the TensorFlow process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        \n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset via tf.keras.dataset\n",
    "Currently, [`tf.keras.dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/datasets) supports 7 datasets. Including:\n",
    "- `mnist` module: MNIST handwritten digits dataset.\n",
    "- `cifar10` module: CIFAR10 small images classification dataset.\n",
    "- `cifar100` module: CIFAR100 small images classification dataset.\n",
    "- `fashion_mnist` module: Fashion-MNIST dataset.\n",
    "- `imdb` module: IMDB sentiment classification dataset.\n",
    "- `boston_housing` module: Boston housing price regression dataset.\n",
    "- `reuters` module: Reuters topic classification dataset.\n",
    "\n",
    "In this lab, we will use [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to demo how to build a neural network. MNIST contains 70,000 images of hand-written digits, 60,000 for training while 10,000 for testing, each $28×28$ pixels, in greyscale with pixel-values from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the MNIST dataset. Convert the samples from integers to floating-point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 28, 28)\n",
      "Test data: (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {}\".format(x_train.shape))\n",
    "print(\"Test data: {}\".format(x_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model via Sequential API\n",
    "### When to use?\n",
    "A `Sequential API` is the simplest way to build a model, which is appropriate for **a plain stack of layers** where each layer has **exactly one input tensor and one output tensor**.\n",
    "\n",
    "Build the [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model by stacking layers. Choose an optimizer and loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example the model returns a vector of \"logits\" or \"log-odds\" scores, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11171019, -0.47011006, -0.481782  , -0.05107722,  0.11246532,\n",
       "         1.1753833 , -0.37382746, -0.34236422, -0.21776126,  0.66296977]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.nn.softmax`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function converts these logits to \"probabilities\" for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07717101, 0.05392661, 0.05330085, 0.08199489, 0.09656338,\n",
       "        0.27953222, 0.05937699, 0.06127489, 0.06940597, 0.16745321]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is possible to bake this `tf.nn.softmax` in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.\n",
    "\n",
    "The [`losses.SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) loss takes a vector of logits and a True index and returns a scalar loss for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss is equal to the negative log probability of the true class: It is zero if the model is sure of the correct class.\n",
    "\n",
    "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.log(1/10) ~= 2.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2746377"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`Model.summary`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary) method prints a string summary of the network, which is quite useful to examining model architecture before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method adjusts the model parameters to minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2956 - accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1437 - accuracy: 0.9574\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1066 - accuracy: 0.9675\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0857 - accuracy: 0.9732\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0738 - accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac3c5f4460>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`Model.evaluate`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) method checks the models performance, usually on a \"Validation-set\" or \"Test-set\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0744 - accuracy: 0.9762 - 837ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07441160082817078, 0.9761999845504761]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image classifier is now trained to ~98% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destroys the current TF graph and creates a new one.\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model via Functional API\n",
    "### When to use?\n",
    "The Keras *Functional API* is a way to create models that are **more flexible** than the `tf.keras.Sequential` API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build *graphs of layers*.\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "```python\n",
    "(input: (28, 28)-dimensional vectors)\n",
    "       ↧\n",
    "[Flatten]\n",
    "       ↧\n",
    "[Dense (128 units, relu activation)]\n",
    "       ↧\n",
    "[Dropout]\n",
    "       ↧\n",
    "[Dense (10 units, softmax activation)]\n",
    "       ↧\n",
    "(output: logits of a probability distribution over 10 classes)\n",
    "```\n",
    "\n",
    "This is a basic graph with four layers. To build this model using the functional API, start by creating an input node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data is set as a 784-dimensional vector. The batch size is always omitted since only the shape of each sample is specified. The `inputs` that is returned contains information about the shape and `dtype` of the input data that you feed to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (None, 28, 28)\n",
      "dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", inputs.shape)\n",
    "print(\"dtype:\", inputs.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a new node in the graph of layers by calling a layer on this `inputs` object. The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer you created. You're \"passing\" the inputs to the `dense` layer, and you get `x` as the output.\n",
    "\n",
    "Let's add a few more layers to the graph of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can create a Model by specifying its inputs and outputs in the graph of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, evaluation, and inference work exactly in the same way for models built using the functional API as for `Sequential` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.3659 - accuracy: 0.8962 - val_loss: 0.1813 - val_accuracy: 0.9498\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.1762 - accuracy: 0.9479 - val_loss: 0.1275 - val_accuracy: 0.9638\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.1332 - accuracy: 0.9612 - val_loss: 0.1111 - val_accuracy: 0.9680\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.1077 - accuracy: 0.9683 - val_loss: 0.0984 - val_accuracy: 0.9715\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0900 - accuracy: 0.9737 - val_loss: 0.0944 - val_accuracy: 0.9723\n",
      "313/313 - 1s - loss: 0.0862 - accuracy: 0.9745 - 759ms/epoch - 2ms/step\n",
      "Test loss: 0.08624877035617828\n",
      "Test accuracy: 0.9745000004768372\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destroys the current TF graph and creates a new one.\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model via Model Subclassing\n",
    "### When to use?\n",
    "The final method to implement a model architecture is called *Model Subclassing*. Model subclassing is **fully-customizable** and enables you to **implement your own custom forward-pass of the model**. However, this flexibility and customization comes at a cost — **model subclassing is way harder to utilize than the Sequential API or Functional API**.\n",
    "\n",
    "So, if the model subclassing method is so hard to use, why bother utilizing it all?\n",
    "\n",
    "Exotic architectures or custom layer/model implementations, **especially those utilized by researchers**, can be extremely challenging, if not impossible, to implement using the standard Sequential or Functional APIs. Instead, researchers wish to have control over every nuance of the network and training process — and that’s exactly what model subclassing provides them.\n",
    "\n",
    "Use [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) to batch and shuffle the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `tf.keras` model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "model.build(input_shape=(None, 28, 28))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an optimizer and loss function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we can use [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) to record operations for automatic differentiation. \n",
    "\n",
    "One of the most important and powerful features of deep learning framework is **automatic differentiation and gradients**. As we can see in [Neural Networks from Scratch](https://nthu-datalab.github.io/ml/labs/10_TensorFlow101/10_NN-from-Scratchs.html), building neural networks manually requires strong knowledge of backpropagation algorithm, where we have to calculate the derivative of everything. It is interesting as we don't have too many operations or the model architecture is relatively simple. When we have tens of millions of operations or much more complicated architecture, computing gradients for each weight will be a nightmare.\n",
    "\n",
    "TensorFlow provides the `tf.GradientTape` API for **automatic differentiation**; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variables`, which is useful for implementing machine learning algorithms such as backpropagation for training neural networks. In short, you can regard `tape.gradient(loss, model.trainable_variable)` as \n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{W_{i,j}}}$$\n",
    "\n",
    "For more details, please refer to [Introduction to Gradients and Automatic Differentiation](https://www.tensorflow.org/guide/autodiff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function decorated by [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will be compiled into a callable TensorFlow graph automatically. This allows the TensorFlow runtime to apply optimizations and exploit parallelism to boost computation performance. We will talk more about `tf.function` in the later section.\n",
    "\n",
    "Let's train and evaluate the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2975, Accuracy: 91.5217, Test Loss: 0.1409, Test Accuracy: 95.7200\n",
      "Epoch 2, Loss: 0.1433, Accuracy: 95.7333, Test Loss: 0.1063, Test Accuracy: 96.7700\n",
      "Epoch 3, Loss: 0.1091, Accuracy: 96.7350, Test Loss: 0.0889, Test Accuracy: 97.2800\n",
      "Epoch 4, Loss: 0.0888, Accuracy: 97.2033, Test Loss: 0.0842, Test Accuracy: 97.5000\n",
      "Epoch 5, Loss: 0.0760, Accuracy: 97.6233, Test Loss: 0.0718, Test Accuracy: 97.8000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    template = 'Epoch {:0}, Loss: {:.4f}, Accuracy: {:.4f}, Test Loss: {:.4f}, Test Accuracy: {:.4f}'\n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100,\n",
    "                           test_loss.result(),\n",
    "                           test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API, Functional API, and Model Subclassing\n",
    "<img src=\"./figs/tf_model.png\" width=\"500\"/>\n",
    "Should you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier and safer, and has a number of features that subclassed models do not support. However, model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass Model directly.\n",
    "\n",
    "Choosing between the functional API or Model subclassing isn't a binary decision that restricts you into one category of models. All models in the `tf.keras` API can interact with each other, whether they're `Sequential` models, functional models, or subclassed models that are written from scratch.\n",
    "\n",
    "For an in-depth look at the differences between the functional API and model subclassing, read [What are Symbolic and Imperative APIs in TensorFlow 2.0?](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better performance with tf.function\n",
    "\n",
    "In TensorFlow 2, **eager execution** is turned on by default. The user interface is intuitive and flexible (running one-off operations is much easier and faster), but this can come at the expense of performance and deployability.\n",
    "\n",
    "You can use [`tf.function`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function) to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. Here we will briefly introduce you how `tf.function` works under the hood so you can use it effectively.\n",
    "\n",
    "The main takeaways and recommendations are:\n",
    "\n",
    "- Debug in eager mode, then decorate with `@tf.function`.\n",
    "- Don't rely on Python side effects like object mutation or list appends.\n",
    "- `tf.function` works best with TensorFlow ops; NumPy and Python calls are converted to constants.\n",
    "\n",
    "Let's create two function with same operation. `f_eager` and `f_graph` represent the functions run in **eager** and **graph** mode, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_eager(x, y):\n",
    "    for i in tf.range(100000):\n",
    "        _ = tf.reduce_mean(tf.multiply(x ** 2, 3) + y)\n",
    "    return tf.reduce_mean(tf.multiply(x ** 2, 3) + y)\n",
    "\n",
    "@tf.function\n",
    "def f_graph(x, y):\n",
    "    for i in tf.range(100000):\n",
    "        _ = tf.reduce_mean(tf.multiply(x ** 2, 3) + y)\n",
    "    return tf.reduce_mean(tf.multiply(x ** 2, 3) + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[2.0, 3.0]])\n",
    "y = tf.constant([[3.0, -2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `f_eager` and `f_graph` return same values, but `f_graph` is executed as a TensorFlow graph.\n",
    "assert f_eager(x, y).numpy() == f_graph(x, y).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 0 ns, total: 23.8 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = f_eager(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 ms, sys: 0 ns, total: 1.56 ms\n",
      "Wall time: 937 µs\n"
     ]
    }
   ],
   "source": [
    "%time _ = f_graph(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, functions decorated with `@tf.function` can be faster than eager code, especially for graphs with many small ops. But for graphs with a few expensive ops (like convolutions), you may not see much speedup.\n",
    "\n",
    "If your code uses multiple functions, you don't need to annotate them all - any functions called from an annotated function will also run in graph mode. Therefore, you only need to decorate the outermost function only.\n",
    "\n",
    "### Debugging\n",
    "In general, debugging code is easier in eager mode than inside `tf.function`. You should ensure that your code executes error-free in eager mode before decorating with `tf.function`. To assist in the debugging process, you can call [`tf.config.run_functions_eagerly(True)`](https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly) to globally disable and reenable tf.function.\n",
    "\n",
    "When tracking down issues that only appear within tf.function, here are some tips:\n",
    "\n",
    "- Plain old Python `print` calls only execute during tracing, helping you track down when your function gets (re)traced.\n",
    "- [`tf.print`](https://www.tensorflow.org/api_docs/python/tf/print) calls will execute every time, and can help you track down intermediate values during execution.\n",
    "- [`tf.debugging.enable_check_numerics`](https://www.tensorflow.org/api_docs/python/tf/debugging/enable_check_numerics) is an easy way to track down where NaNs and Inf are created.\n",
    "- pdb can help you understand what's going on during tracing. (Caveat: PDB will drop you into AutoGraph-transformed source code.)\n",
    "\n",
    "### Python side effects\n",
    "Python side effects like printing, appending to lists, and mutating globals only happen the first time you call a `Function` with a set of inputs. Afterwards, the traced `tf.Graph` is reexecuted, without executing the Python code.\n",
    "\n",
    "The general rule of thumb is to only use Python side effects to debug your traces. Otherwise, TensorFlow ops like `tf.Variable.assign`, `tf.print`, and `tf.summary` are the best way to ensure your code will be traced and executed by the TensorFlow runtime with each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    print(\"Traced with\", x)\n",
    "    tf.print(\"Executed with\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced with 1\n",
      "Executed with 1\n",
      "Executed with 1\n",
      "Traced with 2\n",
      "Executed with 2\n"
     ]
    }
   ],
   "source": [
    "f(1)\n",
    "f(1)\n",
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `print` function works unexpectedly, while `tf.print` calls execute every time. Another gotcha we can see below is mutating globals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  tf.Tensor(1, shape=(), dtype=int32)\n",
      "Second call:  tf.Tensor(2, shape=(), dtype=int32)\n",
      "Third call, different type:  tf.Tensor([14.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = 0\n",
    "\n",
    "@tf.function\n",
    "def mutate_globals(x):\n",
    "    return x + g\n",
    "\n",
    "# tf.function captures the value of the global during the first run\n",
    "print(\"First call: \", mutate_globals(tf.constant(1)))\n",
    "g = 10  # Update the global\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print(\"Second call: \", mutate_globals(tf.constant(2)))\n",
    "\n",
    "# tf.function re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print(\"Third call, different type: \", mutate_globals(tf.constant([4.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, many Python features, such as generators and iterators, rely on the Python runtime to keep track of state. In general, while these constructs work as expected in eager mode, many unexpected things can happen inside a `Function`.\n",
    "\n",
    "For more details about `tf.function` and `Autograph`, including how conditionals and loops work and how to trace code and debug inside `tf.function`, please refer to [tutorial](https://www.tensorflow.org/guide/function) and [video](https://www.youtube.com/watch?v=Up9CvRLIIIw&ab_channel=TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize gradient flow by tf.custom_gradient\n",
    "[`tf.custom_gradient`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/custom_gradient) is a decorator to define a function with a custom gradient. This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.\n",
    "\n",
    "For example, consider the following function that commonly occurs in the computation of cross entropy and log likelihoods:\n",
    "$$y=\\log_e(1+e^x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.math.log(1 + tf.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of y is:\n",
    "\n",
    "$$\\frac{dy}{dx}=\\frac{e^x}{1+e^x}=1-\\frac{1}{1+e^x}$$\n",
    "\n",
    "Theoretically, if x = 100, `dy/dx` should be 1. However, due to numerical instability, the gradient this function evaluated at x=100 is NaN. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = nan\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(100.)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = log1pexp(x)\n",
    "dy = g.gradient(y, x) # Will be evaluated as NaN\n",
    "print(\"dy/dx =\", dy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient expression can be analytically simplified to provide numerical stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.math.log(1 + e), grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 1.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(100.)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = log1pexp(x)\n",
    "dy = g.gradient(y, x) # Will be evaluated as 1.0\n",
    "print(\"dy/dx =\", dy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [TensorFlow](https://www.tensorflow.org/)\n",
    "- [3 ways to create a Keras model with TensorFlow 2.0](https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
