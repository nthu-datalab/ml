{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Neural Networks from Scratch</center>\n",
    "<center> Shan-Hung Wu & DataLab </center>\n",
    "<center> Fall 2022 </center>\n",
    "\n",
    "In this tutorial, you will learn the fundamentals of how you can build neural networks without the help of the deep learning frameworks, and instead by using NumPy.\n",
    "<img src=\"./figs/deep_nn-1.png\" width=\"800\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "Creating complex neural networks with different architectures in Python should be a standard practice for any Machine Learning Engineer and Data Scientist. But a genuine understanding of how a neural network works is equally as valuable. This is what we aim to expand on in this article, the very fundamentals on how we can build neural networks, without the help of the frameworks that make it easy for us.\n",
    "\n",
    "## Model architecture\n",
    "We are building a basic deep neural network with 3 layers in total: 1 input layer, 1 hidden layers and 1 output layer. All layers will be fully connected. We implement ReLU and sigmoid activation functions. SGD and Momentum optimizer are available.\n",
    "\n",
    "Let's try to define the layers in an exact way. To be able to classify digits, we must end up with the probabilities of an image belonging to a certain class, after running the neural network, because then we can quantify how well our neural network performed.\n",
    "\n",
    "1. Input layer: In this layer, we input our dataset, consisting of 28x28 images. We flatten these images into one array with $28×28=784$ elements. This means our input layer will have 784 nodes.\n",
    "2. Hidden layer: In this layer, we have decided to reduce the number of nodes from 784 in the input layer to 64 nodes.\n",
    "3. Output layer: In this layer, we are reducing the 64 nodes to a total of 10 nodes, so that we can evaluate the nodes against the label. This label is received in the form of an array with 10 elements, where one of the elements is 1, while the rest is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "In this tutorial, we will use [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to demo how to build a neural network. MNIST contains 70,000 images of hand-written digits, 60,000 for training while 10,000 for testing, each $28×28$ pixels, in greyscale with pixel-values from 0 to 255.\n",
    "\n",
    "You might notice we not only import Numpy, but also Scikit-learn and Matplotlib. Matplotlib helps us visualize the training data, while Scikit-learn let us load the dataset more easily, but they are not used for any of the actual neural network. There are many ways to load dataset, you can choose any one you prefer.\n",
    "\n",
    "\n",
    "### Note\n",
    "To load dataset, `fetch_openml` requires `scikit-learn >= 0.22`. For older version, please change `from sklearn.datasets import fetch_openml` to `from sklearn.datasets import fetch_mldata`.\n",
    "\n",
    "Now we have to load the dataset and preprocess it, so that we can use it in NumPy. We do normalization by dividing all images by 255, and make it such that all images have values between 0 and 1, since this removes some of the numerical stability issues with activation functions later on. We choose to go with one-hot encoded labels, since we can more easily subtract these labels from the output of the neural network. We also choose to load our inputs as flattened arrays of 28 * 28 = 784 elements, since that is what the input layer requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "mnist_data = fetch_openml(\"mnist_784\")\n",
    "x = np.array(mnist_data[\"data\"])\n",
    "y = np.array(mnist_data[\"target\"])\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_labels = 10\n",
    "examples = y.shape[0]\n",
    "y_new = one_hot(y.astype('int32'), num_labels)\n",
    "\n",
    "# Split, reshape, shuffle\n",
    "train_size = 60000\n",
    "test_size = x.shape[0] - train_size\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y_new[:train_size], y_new[train_size:]\n",
    "shuffle_index = np.random.permutation(train_size)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 784) (60000, 10)\n",
      "Test data: (10000, 784) (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "The specific problem that arises, when trying to implement the feedforward neural network, is that we are trying to transform from 784 nodes all the way down to 10 nodes. When instantiating the `DeepNeuralNetwork` class, we pass in an array of sizes that defines the number of activations for each layer.\n",
    "\n",
    "```python\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10])\n",
    "```\n",
    "\n",
    "This initializes the `DeepNeuralNetwork` class by the init function.\n",
    "\n",
    "```python\n",
    "def __init__(self, sizes, activation='sigmoid'):\n",
    "    self.sizes = sizes\n",
    "\n",
    "    # Choose activation function\n",
    "    if activation == 'relu':\n",
    "        self.activation = self.relu\n",
    "    elif activation == 'sigmoid':\n",
    "        self.activation = self.sigmoid\n",
    "\n",
    "    # Save all weights\n",
    "    self.params = self.initialize()\n",
    "    # Save all intermediate values, i.e. activations\n",
    "    self.cache = {}\n",
    "```\n",
    "\n",
    "To smarten up our initialization, we shrink the variance of the weights in each layer. Following [this nice video](https://www.coursera.org/lecture/deep-neural-network/weight-initialization-for-deep-networks-RwqYe) by Andrew Ng, we’ll set the variance for each layer to 1/n, where n is the number of inputs feeding into that layer. We use `np.random.randn()` function to get our initial weights, which draws from the standard normal distribution. So to adjust the variance to 1/n, we just divide by √n. \n",
    "\n",
    "The initialization of weights in the neural network is kind of hard to think about. To really understand how and why the following approach works, you need a grasp of linear algebra, specifically dimensionality when using the dot product operation, which is beyond the scope of this class.\n",
    "\n",
    "```python\n",
    "def initialize(self):\n",
    "    # number of nodes in each layer\n",
    "    input_layer=self.sizes[0]\n",
    "    hidden_layer=self.sizes[1]\n",
    "    output_layer=self.sizes[2]\n",
    "\n",
    "    params = {\n",
    "        \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "        \"b1\": np.zeros((hidden_layer, 1)),\n",
    "        \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "        \"b2\": np.zeros((output_layer, 1))\n",
    "    }\n",
    "    return params\n",
    "```\n",
    "\n",
    "## Feedforward\n",
    "The forward pass consists of the dot operation in NumPy, which turns out to be just matrix multiplication. As described in the [introduction to neural networks](https://mlfromscratch.com/neural-networks-explained/#/) article, we have to multiply the weights by the activations of the previous layer. Then we have to apply the activation function to the outcome. \n",
    "\n",
    "To get through each layer, we sequentially apply the dot operation, followed by the sigmoid/relu activation function. In the last layer we use the softmax activation function, since we wish to have probabilities of each class, so that we can measure how well our current forward pass performs.\n",
    "\n",
    "<img src=\"./figs/backprop_algo_forward.png\" width=\"300\"/>\n",
    "\n",
    "```python\n",
    "def feed_forward(self, x):\n",
    "    self.cache[\"X\"] = x\n",
    "    self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) +\\\n",
    "                                                        self.params[\"b1\"]\n",
    "    self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "    self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) +\\\n",
    "                                                        self.params[\"b2\"]\n",
    "    self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "    return self.cache[\"A2\"]\n",
    "```\n",
    "\n",
    "The following are the activation functions used for this article. As you can see, we provide a derivative version of the relu and sigmoid, since we will need that later on when backpropagating through the neural network.\n",
    "\n",
    "```python\n",
    "def relu(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def softmax(self, x):\n",
    "    # Numerically stable with large exponentials\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "```\n",
    "\n",
    "Note: A numerical stable version of the softmax function was chosen, you can read more from [Stanford CS231n](https://cs231n.github.io/linear-classify/#softmax) course.\n",
    "\n",
    "## Backpropagation\n",
    "The backward pass is hard to get right, because there are so many sizes and operations that have to align, for all the operations to be successful. Here is the full function for the backward pass; we will go through each weight update below.\n",
    "\n",
    "<img src=\"./figs/backprop_algo_backward.png\" width=\"400\"/>\n",
    "\n",
    "For people who are interested in the magic of backpropagation, please refer to [this nice article](https://mlfromscratch.com/neural-networks-explained/). For people who want to deep dive into methmetics and understand thoroughly, please refer to [NTHU CS565600 Deep Learning](https://nthu-datalab.github.io/ml/index.html). Here is [slide](https://nthu-datalab.github.io/ml/slides/10_NN_Design.pdf) and [video](https://www.youtube.com/watch?v=uYRUbvyKXAo&list=PLlPcwHqLqJDk3A0qFgFUDlyALzaF44NTL&index=4&ab_channel=Shan-HungWu) for backpropagation.\n",
    "\n",
    "```python\n",
    "def back_propagate(self, y, output):\n",
    "    current_batch_size = y.shape[0]\n",
    "\n",
    "    dZ2 = output - y.T\n",
    "    dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "    db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "    dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "    db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "    return self.grads\n",
    "```\n",
    "\n",
    "## Training (Stochastic Gradient Descent)\n",
    "We have defined a forward and backward pass, but how can we start using them? We have to make a training loop and choose to use Stochastic Gradient Descent (SGD) as the optimizer to update the parameters of the neural network.\n",
    "\n",
    "There are two main loops in the training function. One loop for the number of epochs, which is the number of times we run through the whole dataset, and a second loop for running through each batch one by one.\n",
    "\n",
    "For each batch, we do a forward pass by calling `self.feedforward()` with `x`, which is one batch in an array with the length 784, as explained earlier. The `output` of the forward pass is used along with `y`, which are the one-hot encoded labels (the ground truth), in the backward pass. `self.back_propagate()` returns the gradients of weights.\n",
    "\n",
    "```python\n",
    "def train(self, x_train, y_train, x_test, y_test):\n",
    "    for i in range(self.epochs):\n",
    "        # Shuffle\n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            # Batch\n",
    "            begin = j * self.batch_size\n",
    "            end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "            x = x_train_shuffled[begin:end]\n",
    "            y = y_train_shuffled[begin:end]\n",
    "\n",
    "            # Forward\n",
    "            output = self.feed_forward(x)\n",
    "            # Backprop\n",
    "            grad = self.back_propagate(y, output)\n",
    "            # Optimize\n",
    "            self.optimize(l_rate=l_rate, beta=beta)\n",
    "```\n",
    "\n",
    "The `optimize()` function has the code for both SGD and momentum update rules. SGD algorithm is relatively straightforward, updating the networks by calculated gradient directly, while momentum make the same movement in the last iteration, corrected by negative gradient.\n",
    "\n",
    "```python\n",
    "def optimize(self, l_rate=0.1, beta=.9):\n",
    "    '''\n",
    "        Stochatic Gradient Descent (SGD):\n",
    "        θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "        Momentum:\n",
    "        v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "        θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "    '''\n",
    "    if self.optimizer == \"sgd\":\n",
    "        for key in self.params:\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate*self.grads[key]\n",
    "    elif self.optimizer == \"momentum\":\n",
    "        for key in self.params:\n",
    "            self.momemtum_opt[key] = (beta*self.momemtum_opt[key] +\\\n",
    "                                      (1.-beta)*self.grads[key])\n",
    "            self.params[key] = self.params[key] -\\\n",
    "                                        l_rate * self.momemtum_opt[key]\n",
    "\n",
    "```\n",
    "\n",
    "After having updated the parameters of the neural network, we can measure the accuracy on a validation set that we conveniently prepared earlier, to validate how well our network performs after each iteration over the whole dataset.\n",
    "\n",
    "```python\n",
    "def accuracy(self, y, output):\n",
    "    return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "```\n",
    "\n",
    "Finally, we can call the training function, after knowing what will happen. We use the training and validation data as input to the training function, and then we wait.\n",
    "\n",
    "```python\n",
    "dnn.train(x_train, y_train, x_val, y_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer neural network with Numpy\n",
    "Here is the full code, for an easy copy-paste and overview of what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        \n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "            \n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialize(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        \n",
    "        params = {\n",
    "            \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "            \"b1\": np.zeros((hidden_layer, 1)),\n",
    "            \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "            \"b2\": np.zeros((output_layer, 1))\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momemtum_opt = {\n",
    "            \"W1\": np.zeros(self.params[\"W1\"].shape),\n",
    "            \"b1\": np.zeros(self.params[\"b1\"].shape),\n",
    "            \"W2\": np.zeros(self.params[\"W2\"].shape),\n",
    "            \"b2\": np.zeros(self.params[\"b2\"].shape),\n",
    "        }\n",
    "        return momemtum_opt\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "            y = σ(wX + b)\n",
    "        '''\n",
    "        self.cache[\"X\"] = x\n",
    "        self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) + self.params[\"b1\"]\n",
    "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "        self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) + self.params[\"b2\"]\n",
    "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "        return self.cache[\"A2\"]\n",
    "    \n",
    "    def back_propagate(self, y, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        current_batch_size = y.shape[0]\n",
    "        \n",
    "        dZ2 = output - y.T\n",
    "        dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "        dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m = y.shape[0]\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "                \n",
    "    def optimize(self, l_rate=0.1, beta=.9):\n",
    "        '''\n",
    "            Stochatic Gradient Descent (SGD):\n",
    "            θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "            \n",
    "            Momentum:\n",
    "            v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "            θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "        '''\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                self.params[key] = self.params[key] - l_rate * self.grads[key]\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for key in self.params:\n",
    "                self.momemtum_opt[key] = (beta * self.momemtum_opt[key] + (1. - beta) * self.grads[key])\n",
    "                self.params[key] = self.params[key] - l_rate * self.momemtum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer is currently not support, please use 'sgd' or 'momentum' instead.\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=10, \n",
    "              batch_size=64, optimizer='momentum', l_rate=0.1, beta=.9):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
    "        \n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_test)\n",
    "            test_acc = self.accuracy(y_test, output)\n",
    "            test_loss = self.cross_entropy_loss(y_test, output)\n",
    "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The results completely dependent on how the weights are initialized and the activation function we use. Experimentally, due to non-bounded behavior of `relu()`, the learning rate should be set much smaller than the one for `sigmoid()` (bounded). In addition, training with SGD optimizer with momentum should have better result since it avoids from getting stuck in local minima or saddle points.\n",
    "\n",
    "The reason behind this phenomenon is complicated and beyond the scope of this class. In short, the training results will be more stable and consistent as the batch size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.70s, train acc=0.95, train loss=0.16, test acc=0.95, test loss=0.17\n",
      "Epoch 2: 1.45s, train acc=0.97, train loss=0.11, test acc=0.96, test loss=0.13\n",
      "Epoch 3: 2.19s, train acc=0.98, train loss=0.08, test acc=0.97, test loss=0.10\n",
      "Epoch 4: 2.92s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.10\n",
      "Epoch 5: 3.64s, train acc=0.98, train loss=0.05, test acc=0.97, test loss=0.09\n",
      "Epoch 6: 4.47s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.09\n",
      "Epoch 7: 5.21s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.09\n",
      "Epoch 8: 5.99s, train acc=0.99, train loss=0.03, test acc=0.97, test loss=0.09\n",
      "Epoch 9: 6.76s, train acc=0.99, train loss=0.03, test acc=0.97, test loss=0.09\n",
      "Epoch 10: 7.49s, train acc=0.99, train loss=0.03, test acc=0.97, test loss=0.09\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid + Momentum\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='momentum', l_rate=4, beta=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.61s, train acc=0.89, train loss=0.41, test acc=0.89, test loss=0.39\n",
      "Epoch 2: 1.22s, train acc=0.90, train loss=0.33, test acc=0.91, test loss=0.31\n",
      "Epoch 3: 1.88s, train acc=0.91, train loss=0.30, test acc=0.92, test loss=0.29\n",
      "Epoch 4: 2.47s, train acc=0.92, train loss=0.29, test acc=0.92, test loss=0.28\n",
      "Epoch 5: 3.06s, train acc=0.92, train loss=0.28, test acc=0.92, test loss=0.27\n",
      "Epoch 6: 3.66s, train acc=0.92, train loss=0.27, test acc=0.92, test loss=0.26\n",
      "Epoch 7: 4.26s, train acc=0.92, train loss=0.26, test acc=0.93, test loss=0.26\n",
      "Epoch 8: 4.85s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n",
      "Epoch 9: 5.46s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n",
      "Epoch 10: 6.05s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n"
     ]
    }
   ],
   "source": [
    "# ReLU + SGD\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good exercises in NumPy\n",
    "You might have noticed that the code is very readable, but takes up a lot of space and could be optimized to run in loops. Here is a chance to optimize and improve the code. For example, you can optimize the forward and backward pass, such that they run in a for loop in each function. This makes the code easier to modify and possibly easier to maintain. \n",
    "\n",
    "More challenging exercises including implement any other activation function from this overview of activation functions, and remember to implement the derivatives as well. Different optimizers, e.g. Adam, RMSProp, etc, are also worth to try.\n",
    "\n",
    "## Reference \n",
    "1. [CS565600 Deep Learning](https://nthu-datalab.github.io/ml/index.html), National Tsing Hua University\n",
    "2. [Building a Neural Network from Scratch: Part 1](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/)\n",
    "3. [Building a Neural Network from Scratch: Part 2](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/)\n",
    "4. [Neural networks from scratch](https://developer.ibm.com/technologies/artificial-intelligence/articles/neural-networks-from-scratch), IBM Developer\n",
    "5. [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
