{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks from scratch\n",
    "In this tutorial, you will learn the fundamentals of how you can build neural networks without the help of the deep learning frameworks, and instead by using NumPy.\n",
    "<img src=\"./figs/nn_thumbnail.png\" width=\"800\"/>\n",
    "\n",
    "*This article was first published by IBM Developer at [developer.ibm.com](https://developer.ibm.com/technologies/artificial-intelligence/articles/neural-networks-from-scratch), authored by Casper Hansen.*\n",
    "\n",
    "Creating complex neural networks with different architectures in Python should be a standard practice for any Machine Learning Engineer and Data Scientist. But a genuine understanding of how a neural network works is equally as valuable. This is what we aim to expand on in this article, the very fundamentals on how we can build neural networks, without the help of the frameworks that make it easy for us.\n",
    "\n",
    "## Numpy\n",
    "We are building a basic deep neural network with 4 layers in total: 1 input layer, 2 hidden layers and 1 output layer. All layers will be fully connected.\n",
    "\n",
    "For training the neural network, we will use stochastic gradient descent; which means we put one image through the neural network at a time.\n",
    "\n",
    "Let's try to define the layers in an exact way. To be able to classify digits, we must end up with the probabilities of an image belonging to a certain class, after running the neural network, because then we can quantify how well our neural network performed.\n",
    "\n",
    "1. Input layer: In this layer, we input our dataset, consisting of 28x28 images. We flatten these images into one array with $28×28=784$ elements. This means our input layer will have 784 nodes.\n",
    "2. Hidden layer 1: In this layer, we have decided to reduce the number of nodes from 784 in the input layer to 128 nodes. This brings a challenge when we are going forward in the neural network (explained later).\n",
    "3. Hidden layer 2: In this layer, we have decided to go with 64 nodes, from the 128 nodes in the first hidden layer. This is no new challenge, since we already reduced the number in the first layer.\n",
    "4. Output layer: In this layer, we are reducing the 64 nodes to a total of 10 nodes, so that we can evaluate the nodes against the label. This label is received in the form of an array with 10 elements, where one of the elements is 1, while the rest is 0.\n",
    "\n",
    "<img src=\"./figs/deep_nn-1.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading with `tensorflow/datasets`\n",
    "You might notice we not only import Numpy, but also Matplotlib and TensorFlow dataset. Matplotlib helps us visualize the training data, while TensorFlow dataset let us load the dataset more easily, but they are not used for any of the actual neural network.\n",
    "\n",
    "Now we have to load the dataset and preprocess it, so that we can use it in NumPy. We do normalization by dividing all images by 255, and make it such that all images have values between 0 and 1, since this removes some of the numerical stability issues with activation functions later on. We choose to go with one-hot encoded labels, since we can more easily subtract these labels from the output of the neural network. We also choose to load our inputs as flattened arrays of 28 * 28 = 784 elements, since that is what the input layer requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "\n",
    "# Full train set\n",
    "x_train, y_train = train_data['image'], train_data['label']\n",
    "x_train = np.reshape(x_train, (len(x_train), -1)) / 255.0\n",
    "y_train = one_hot(y_train, num_labels)\n",
    "\n",
    "# Full test set\n",
    "x_test, y_test = test_data['image'], test_data['label']\n",
    "x_test = np.reshape(x_test, (len(x_test), -1)) / 255.0\n",
    "y_test = one_hot(y_test, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 784) (60000, 10)\n",
      "Test data: (10000, 784) (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "The initialization of weights in the neural network is kind of hard to think about. To really understand how and why the following approach works, you need a grasp of linear algebra, specifically dimensionality when using the dot product operation.\n",
    "\n",
    "The specific problem that arises, when trying to implement the feedforward neural network, is that we are trying to transform from 784 nodes all the way down to 10 nodes. When instantiating the `DeepNeuralNetwork` class, we pass in an array of sizes that defines the number of activations for each layer.\n",
    "\n",
    "```python\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
    "```\n",
    "\n",
    "This initializes the `DeepNeuralNetwork` class by the init function.\n",
    "\n",
    "```python\n",
    "def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
    "    self.sizes = sizes\n",
    "    self.epochs = epochs\n",
    "    self.l_rate = l_rate\n",
    "\n",
    "    # We save all parameters in the neural network in this dictionary\n",
    "    self.params = self.initialization()\n",
    "```\n",
    "\n",
    "Let's look at how the sizes affect the parameters of the neural network, when calling the `initialization()` function. We are preparing $m x n$ matrices that are *\"dot-able\"*, so that we can do a forward pass, while shrinking the number of activations as the layers increase. We can only use the dot product operation for two matrices `M1` and `M2`, where $m$ in `M1` is equal to $n$ in `M2`, or where $n$ in `M1` is equal to $m$ in `M2`.\n",
    "\n",
    "With this explanation, you can see that we initialize the first set of weights `W1` with $m = 128$ and $n = 784$, while the next weights `W2` are  $m = 64$ and  $n = 128$. The number of activations in the input layer A0 is equal to 784, as explained earlier, and when we dot W1 by the activations `A0`, the operation is successful.\n",
    "\n",
    "```python\n",
    "def initialization(self):\n",
    "    # Number of nodes in each layer\n",
    "    input_layer=self.sizes[0]\n",
    "    hidden_1=self.sizes[1]\n",
    "    hidden_2=self.sizes[2]\n",
    "    output_layer=self.sizes[3]\n",
    "\n",
    "    params = {\n",
    "        'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "        'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "    }\n",
    "\n",
    "    return params\n",
    "```\n",
    "\n",
    "## Feedforward\n",
    "The forward pass consists of the dot operation in NumPy, which turns out to be just matrix multiplication. As described in the [introduction to neural networks](https://mlfromscratch.com/neural-networks-explained/#/) article, we have to multiply the weights by the activations of the previous layer. Then we have to apply the activation function to the outcome.\n",
    "\n",
    "To get through each layer, we sequentially apply the dot operation, followed by the sigmoid activation function. In the last layer we use the softmax activation function, since we wish to have probabilities of each class, so that we can measure how well our current forward pass performs.\n",
    "\n",
    "Note: A numerical stable version of the softmax function was chosen, you can read more from [Stanford CS231n](https://cs231n.github.io/linear-classify/#softmax) course.\n",
    "\n",
    "```python\n",
    "def forward_pass(self, x_train):\n",
    "    params = self.params\n",
    "\n",
    "    # Input layer activations becomes sample\n",
    "    params['A0'] = x_train\n",
    "\n",
    "    # Input layer to hidden layer 1\n",
    "    params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "    params['A1'] = self.sigmoid(params['Z1'])\n",
    "\n",
    "    # Hidden layer 1 to hidden layer 2\n",
    "    params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "    params['A2'] = self.sigmoid(params['Z2'])\n",
    "\n",
    "    # Hidden layer 2 to output layer\n",
    "    params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "    params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "    return params['A3']\n",
    "```\n",
    "\n",
    "The following are the activation functions used for this article. As can be observed, we provide a derivative version of the relu and sigmoid, since we will need that later on when backpropagating through the neural network.\n",
    "\n",
    "```python\n",
    "def relu(self, x, derivative=False):\n",
    "    # Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "    #\n",
    "    # Forward path:\n",
    "    # relu(x) = max(0, x)\n",
    "    # In other word,\n",
    "    # relu(x) = 0, if x < 0\n",
    "    #         = x, if x >= 0\n",
    "    #\n",
    "    # Backward path:\n",
    "    # ∇relu(x) = 0, if x < 0\n",
    "    #          = 1, if x >=0\n",
    "    if derivative:\n",
    "        if x < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(self, x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def softmax(self, x):\n",
    "    # Numerically stable with large exponentials\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "```\n",
    "\n",
    "## Backpropagation\n",
    "The backward pass is hard to get right, because there are so many sizes and operations that have to align, for all the operations to be successful. Here is the full function for the backward pass; we will go through each weight update below.\n",
    "\n",
    "```python\n",
    "def backward_pass(self, y_train, output):\n",
    "    '''\n",
    "        This is the backpropagation algorithm, for calculating the updates\n",
    "        of the neural network's parameters.\n",
    "    '''\n",
    "    params = self.params\n",
    "    change_w = {}\n",
    "\n",
    "    # Calculate W3 update\n",
    "    error = output - y_train\n",
    "    change_w['W3'] = np.dot(error, params['A3'])\n",
    "\n",
    "    # Calculate W2 update\n",
    "    error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
    "    change_w['W2'] = np.dot(error, params['A2'])\n",
    "\n",
    "    # Calculate W1 update\n",
    "    error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
    "    change_w['W1'] = np.dot(error, params['A1'])\n",
    "\n",
    "    return change_w\n",
    "```\n",
    "\n",
    "### W3 update\n",
    "The update for `W3` can be calculated by subtracting the ground truth array with labels called `y_train` from the output of the forward pass called `output`. This operation is successful, because `len(y_train)` is 10 and `len(output)` is also 10.\n",
    "\n",
    "An example of `y_train` might be the following, where the 1 is corresponding to the label of the output:\n",
    "```python\n",
    "y_train = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "```\n",
    "While an example of `output` might be the following, where the numbers are probabilities corresponding to the classes of `y_train`:\n",
    "```\n",
    "output = np.array([0.2, 0.2, 0.5, 0.3, 0.6, 0.4, 0.2, 0.1, 0.3, 0.7])\n",
    "```\n",
    "If we subtract them, we get the following:\n",
    "```python\n",
    ">>> output - y_train\n",
    "array([ 0.2,  0.2, -0.5,  0.3,  0.6,  0.4,  0.2,  0.1,  0.3,  0.7])\n",
    "```\n",
    "The next operation is the dot operation that dots the error (which we just calculated) with the activations of the last layer.\n",
    "```python\n",
    "error = output - y_train\n",
    "change_w['W3'] = np.dot(error, params['A3'])\n",
    "```\n",
    "\n",
    "### W2 update\n",
    "The next is updating the weights `W2`. More operations are involved for success. Firstly, there is a slight mismatch in shapes, because `W3` has the shape `(10, 64)`, and `error` has `(10, 64)`, i.e. the exact same dimensions. Thus, we can use a [transpose operation](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html#numpy.transpose) on the `W3` parameter by the `.T`, such that the array has its dimensions permuted and the shapes now align up for the dot operation.\n",
    "\n",
    "`W3` now has shape `(64, 10)` and `error` has shape `(10, 64)`, which are compatible with the dot operation. The result is [multiplied element-wise](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) (also called Hadamard product) with the outcome of the derivative of the sigmoid function of `Z2`. At last, we dot the error with the activations of the previous layer.\n",
    "\n",
    "```python\n",
    "error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
    "change_w['W2'] = np.dot(error, params['A2'])\n",
    "```\n",
    "\n",
    "### W1 update\n",
    "Likewise, the code for updating `W1` is using the parameters of the neural network one step earlier. Except for other parameters, the code is equivalent to the `W2` update.\n",
    "\n",
    "```python\n",
    "error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
    "change_w['W1'] = np.dot(error, params['A1'])\n",
    "```\n",
    "\n",
    "## Training (Stochastic Gradient Descent)\n",
    "We have defined a forward and backward pass, but how can we start using them? We have to make a training loop and choose to use Stochastic Gradient Descent (SGD) as the optimizer to update the parameters of the neural network.\n",
    "\n",
    "There are two main loops in the training function. One loop for the number of epochs, which is the number of times we run through the whole dataset, and a second loop for running through each observation one by one.\n",
    "\n",
    "For each observation, we do a forward pass with `x`, which is one image in an array with the length 784, as explained earlier. The `output` of the forward pass is used along with `y`, which are the one-hot encoded labels (the ground truth), in the backward pass. This gives us a dictionary of updates to the weights in the neural network.\n",
    "\n",
    "```python\n",
    "def train(self, x_train, y_train, x_val, y_val):\n",
    "    start_time = time.time()\n",
    "    for iteration in range(self.epochs):\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            output = self.forward_pass(x)\n",
    "            changes_to_w = self.backward_pass(y, output)\n",
    "            self.update_network_parameters(changes_to_w)\n",
    "        \n",
    "        accuracy = self.compute_accuracy(x_val, y_val)\n",
    "        print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2}'.format(\n",
    "            iteration+1, time.time() - start_time, accuracy\n",
    "        ))\n",
    "```\n",
    "\n",
    "The `update_network_parameters()` function has the code for the SGD update rule, which just needs the gradients for the weights as input. And to be clear, SGD involves calculating the gradient using backpropagation from the backward pass, not just updating the parameters. They seem separate and they should be thought of separately, since the two algorithms are different.\n",
    "\n",
    "```python\n",
    "def update_network_parameters(self, changes_to_w):\n",
    "    '''\n",
    "        Update network parameters according to update rule from\n",
    "        Stochastic Gradient Descent.\n",
    "\n",
    "        θ = θ - η * ∇J(x, y), \n",
    "            theta θ:            a network parameter (e.g. a weight w)\n",
    "            eta η:              the learning rate\n",
    "            gradient ∇J(x, y):  the gradient of the objective function,\n",
    "                                i.e. the change for a specific theta θ\n",
    "    '''\n",
    "    \n",
    "    for key, value in changes_to_w.items():\n",
    "        for w_arr in self.params[key]:\n",
    "            w_arr -= self.l_rate * value\n",
    "```\n",
    "\n",
    "After having updated the parameters of the neural network, we can measure the accuracy on a validation set that we conveniently prepared earlier, to validate how well our network performs after each iteration over the whole dataset.\n",
    "\n",
    "This code uses some of the same pieces as the training function; to begin with, it does a forward pass, then it finds the prediction of the network and checks for equality with the label. After that, we sum over the predictions and divide by 100 to find the accuracy, and at last, we average out the accuracy of each class.\n",
    "\n",
    "```python\n",
    "def compute_accuracy(self, x_val, y_val):\n",
    "    '''\n",
    "        This function does a forward pass of x, then checks if the indices\n",
    "        of the maximum value in the output equals the indices in the label\n",
    "        y. Then it sums over each prediction and calculates the accuracy.\n",
    "    '''\n",
    "    predictions = []\n",
    "\n",
    "    for x, y in zip(x_val, y_val):\n",
    "        output = self.forward_pass(x)\n",
    "        pred = np.argmax(output)\n",
    "        predictions.append(pred == y)\n",
    "    \n",
    "    summed = sum(pred for pred in predictions) / 100.0\n",
    "    return np.average(summed)\n",
    "```\n",
    "\n",
    "Finally, we can call the training function, after knowing what will happen. We use the training and validation data as input to the training function, and then we wait.\n",
    "\n",
    "```python\n",
    "dnn.train(x_train, y_train, x_val, y_val)\n",
    "```\n",
    "\n",
    "Note that the results may vary a lot, depending on how the weights are initialized. My results range from an accuracy of 0%, and all the way to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-layer neural network with Numpy\n",
    "Here is the full code, for an easy copy-paste and overview of what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=10, l_rate=0.001, activation='relu'):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # We save all parameters in the neural network in this dictionary\n",
    "        self.params = self.initialization()\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        # Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        #\n",
    "        # Forward path:\n",
    "        # relu(x) = max(0, x)\n",
    "        # In other word,\n",
    "        # relu(x) = 0, if x < 0\n",
    "        #         = x, if x >= 0\n",
    "        #\n",
    "        # Backward path:\n",
    "        # ∇relu(x) = 0, if x < 0\n",
    "        #          = 1, if x >=0\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        params = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "\n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train\n",
    "        \n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "        params['A1'] = self.activation(params['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "        params['A2'] = self.activation(params['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "        params['A3'] = params['Z3']\n",
    "#         params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        # Calculate W3 update\n",
    "        error = output - y_train\n",
    "#         change_w['W3'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A2'], (params['A2'].shape[0], 1)).T)\n",
    "        change_w['W3'] = np.dot(error, params['A3'])\n",
    "\n",
    "        # Calculate W2 update\n",
    "        error = np.multiply(np.dot(params['W3'].T, error), dnn.activation(params['Z2'], derivative=True) )\n",
    "#         change_w['W2'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A1'], (params['A1'].shape[0], 1)).T)\n",
    "        change_w['W2'] = np.dot(error, params['A2'])\n",
    "        \n",
    "        # Calculate W1 update\n",
    "        error = np.multiply(np.dot(params['W2'].T, error), dnn.activation(params['Z1'], derivative=True) )\n",
    "#         change_w['W1'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A0'], (params['A0'].shape[0], 1)).T)\n",
    "        change_w['W1'] = np.dot(error, params['A1'])\n",
    "        return change_w\n",
    "    \n",
    "    def mse_loss(self, y_train, output):\n",
    "        return np.sum((output - y_train)**2) / 2\n",
    "    \n",
    "    def cross_entropy_loss(self, y_train, output):\n",
    "        return -np.sum(np.log(output)*y_train)\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        '''\n",
    "            Update network parameters according to update rule from\n",
    "            Stochastic Gradient Descent.\n",
    "\n",
    "            θ = θ - η * ∇J(x, y), \n",
    "                theta θ:            a network parameter (e.g. a weight w)\n",
    "                eta η:              the learning rate\n",
    "                gradient ∇J(x, y):  the gradient of the objective function,\n",
    "                                    i.e. the change for a specific theta θ\n",
    "        '''\n",
    "        \n",
    "        for key, value in changes_to_w.items():\n",
    "            # Gradient clipping\n",
    "#             value = np.clip(value, -1, 1)\n",
    "#             self.params[key] -= self.l_rate * value\n",
    "            for w_arr in self.params[key]:\n",
    "                w_arr -= self.l_rate * value\n",
    "\n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        '''\n",
    "            This function does a forward pass of x, then checks if the indices\n",
    "            of the maximum value in the output equals the indices in the label\n",
    "            y. Then it sums over each prediction and calculates the accuracy.\n",
    "        '''\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == y)\n",
    "        \n",
    "        summed = sum(pred for pred in predictions) / 100.0\n",
    "        return np.average(summed)\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, batch_size=1):\n",
    "        start_time = time.time()\n",
    "        changes_tmp = {'W1': 0, 'W2': 0, 'W3': 0}\n",
    "        \n",
    "        for iteration in range(self.epochs):\n",
    "            x_train, y_train = self.shuffle(x_train, y_train)\n",
    "#             self.l_rate /= 10\n",
    "#             print(self.l_rate)\n",
    "            \n",
    "            for idx, (x, y) in enumerate(zip(x_train, y_train)):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                \n",
    "                # Batch SGD\n",
    "                for k, v in changes_to_w.items():\n",
    "                    changes_tmp[k] += v\n",
    "                if (idx+1) % batch_size == 0:\n",
    "                    # Normalize gradient\n",
    "                    for k, v in changes_tmp.items():\n",
    "                        changes_tmp[k] /= batch_size\n",
    "                    # Update\n",
    "                    self.update_network_parameters(changes_tmp)\n",
    "                    # Set back to 0\n",
    "                    for k, v in changes_tmp.items():\n",
    "                        changes_tmp[k] = 0\n",
    "            \n",
    "            accuracy = self.compute_accuracy(x_val, y_val)\n",
    "            print('Epoch: {}, Time Spent: {:.2f}s, Accuracy: {:.2f}'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy\n",
    "            ))\n",
    "            \n",
    "    def shuffle(self, x, y):\n",
    "        indices = np.arange(x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        return x[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The results completely dependent on how the weights are initialized and the activation function we use. Sometimes we are stuck at 0% accuracy, sometimes 5-10%, other times it jumps from 22% to 94.5%. Experimentally, using `relu` as an activation function is much easier to train than using `sigmoid` function.\n",
    "\n",
    "The reason behind this phenomenon is complicated and beyond the scope of this class. In short, the training results will be more stable and consistent as the batch size increases, where in the above code, the batch size is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 5.34s, Accuracy: 90.00\n",
      "Epoch: 2, Time Spent: 10.16s, Accuracy: 90.00\n",
      "Epoch: 3, Time Spent: 15.22s, Accuracy: 90.00\n",
      "Epoch: 4, Time Spent: 19.85s, Accuracy: 90.00\n",
      "Epoch: 5, Time Spent: 24.96s, Accuracy: 90.00\n",
      "Epoch: 6, Time Spent: 30.13s, Accuracy: 90.00\n",
      "Epoch: 7, Time Spent: 35.34s, Accuracy: 90.00\n",
      "Epoch: 8, Time Spent: 40.48s, Accuracy: 90.00\n",
      "Epoch: 9, Time Spent: 45.30s, Accuracy: 90.00\n",
      "Epoch: 10, Time Spent: 50.02s, Accuracy: 90.00\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10429339, 0.09345395, 0.11552957, 0.12871619, 0.07025013,\n",
       "       0.10412232, 0.09407901, 0.11122855, 0.08445142, 0.09387548])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.forward_pass(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.04255693,  0.11762898,  0.00255927, ...,  0.07772373,\n",
       "          0.02292998,  0.10135429],\n",
       "        [-0.00306626, -0.06121665,  0.05046053, ...,  0.04281437,\n",
       "         -0.03993251,  0.01941595],\n",
       "        [ 0.02153047,  0.029231  , -0.04560757, ..., -0.06223476,\n",
       "         -0.07058069, -0.02317047],\n",
       "        ...,\n",
       "        [-0.0792573 , -0.04805836,  0.10485631, ..., -0.0731238 ,\n",
       "          0.00050823,  0.01866119],\n",
       "        [ 0.0526039 ,  0.13026234, -0.03778257, ...,  0.02763853,\n",
       "          0.04548242, -0.18136042],\n",
       "        [-0.16489186,  0.07102633,  0.05596752, ...,  0.09370558,\n",
       "         -0.04379449,  0.04059317]]),\n",
       " 'W2': array([[ 0.04307908,  0.0282143 , -0.12124998, ..., -0.22078609,\n",
       "          0.03756114, -0.00441314],\n",
       "        [ 0.0027941 , -0.06059902, -0.06736848, ..., -0.05805212,\n",
       "         -0.09897825, -0.07339157],\n",
       "        [ 0.06571235, -0.04854012,  0.00049104, ...,  0.06061818,\n",
       "          0.1003069 , -0.18486332],\n",
       "        ...,\n",
       "        [ 0.00953598,  0.04162815,  0.01693728, ..., -0.13690762,\n",
       "         -0.314925  ,  0.02423588],\n",
       "        [ 0.19421306,  0.06026207,  0.00254554, ..., -0.09680773,\n",
       "          0.13820348, -0.25021029],\n",
       "        [-0.09354872, -0.17125767, -0.00551695, ...,  0.06425774,\n",
       "          0.0800256 ,  0.15496839]]),\n",
       " 'W3': array([[ 1.17655810e-01,  5.97358008e-01, -5.67815465e-01,\n",
       "         -9.92124882e-01, -2.64072063e-01,  1.83121376e-01,\n",
       "         -1.92606918e-01,  1.65315039e-01,  1.20351817e-01,\n",
       "         -1.68286483e-01,  4.18764734e-03, -2.78623152e-01,\n",
       "          3.90371289e-01, -1.92979999e-01,  1.37496897e-01,\n",
       "         -2.16115583e-01, -1.22309291e-01, -7.87755033e-02,\n",
       "          7.29316339e-02,  4.79258297e-01, -1.97207140e-01,\n",
       "         -3.94922129e-01, -4.60939873e-01, -3.12569626e-02,\n",
       "         -2.65126506e-01, -3.96794587e-01, -3.29274276e-01,\n",
       "          8.34503824e-02,  3.90616095e-01, -3.66196218e-01,\n",
       "         -6.96560859e-01,  1.06455027e-01, -1.93684934e-01,\n",
       "         -6.93852120e-01, -6.11527718e-01,  7.11868154e-02,\n",
       "          2.00152442e-01,  4.73449365e-02,  5.71785736e-01,\n",
       "          8.15898807e-02,  8.56999593e-01, -2.98893831e-01,\n",
       "         -6.19437292e-01, -3.92599544e-02,  9.61610664e-02,\n",
       "          1.00438148e-01, -3.20470265e-01,  2.84440462e-01,\n",
       "         -2.62292817e-01,  6.68132898e-02,  3.16758344e-01,\n",
       "         -6.64194155e-02, -4.04866680e-01,  6.94395931e-02,\n",
       "          1.73244062e-01, -4.11931608e-01, -2.60129418e-01,\n",
       "         -3.10572657e-01, -1.86743436e-01, -1.86038515e-01,\n",
       "         -1.23886838e-01, -3.52056274e-01, -1.27603002e-01,\n",
       "         -4.14283216e-01],\n",
       "        [ 5.70626115e-01,  5.94355288e-01,  3.62370874e-01,\n",
       "          4.09912230e-01, -8.99066880e-02,  3.83183126e-01,\n",
       "          8.64208470e-01, -1.43299627e-02,  1.16888793e-01,\n",
       "          1.74537390e-01,  2.56761855e-01,  3.04224024e-01,\n",
       "          1.14015400e-01,  1.96998660e-01, -2.48655853e-01,\n",
       "          4.01052323e-01,  5.63115842e-01, -1.33725294e-01,\n",
       "          4.01593079e-01,  1.18843543e-01,  7.09292621e-02,\n",
       "          1.97242970e-03,  4.09376530e-01,  2.28924551e-01,\n",
       "         -3.99043925e-01, -1.64339557e-01,  6.69246329e-01,\n",
       "          3.92554880e-01, -5.92749626e-01, -1.24497857e-01,\n",
       "         -2.78126044e-01,  6.05441021e-01,  9.19325995e-02,\n",
       "         -1.88491093e-01,  2.26090034e-01, -2.41260231e-01,\n",
       "         -2.68532777e-01, -1.04743279e-02,  4.27722115e-01,\n",
       "          1.83381098e-01, -2.75386636e-01,  3.50277366e-01,\n",
       "          1.40656090e-01, -2.97832970e-01,  4.13450972e-02,\n",
       "          1.71649554e-01, -6.13370426e-02,  1.15186704e-01,\n",
       "          2.49816220e-01, -2.46976325e-01, -5.72685788e-01,\n",
       "         -3.14487934e-01,  1.21705206e-01,  2.92185033e-01,\n",
       "         -6.02823201e-02,  2.92706781e-01,  3.87480302e-01,\n",
       "         -2.76844438e-01,  2.36550665e-01, -1.79149442e-02,\n",
       "          2.34235503e-01,  2.67073729e-02,  1.33975321e-01,\n",
       "          4.92530782e-01],\n",
       "        [-1.98429518e-02,  3.32056566e-01,  2.90918154e-01,\n",
       "          3.17329562e-01, -1.27553408e-01, -2.07832435e-02,\n",
       "         -4.80722978e-02,  6.83228774e-01, -3.21020983e-01,\n",
       "          4.02921025e-01,  3.33673490e-02,  1.74752791e-01,\n",
       "         -6.83838017e-02, -9.35574894e-02, -5.50486053e-01,\n",
       "         -3.04135396e-01, -4.42182583e-01, -3.34843048e-02,\n",
       "         -3.10870957e-01,  1.58981942e-01,  9.71348992e-02,\n",
       "          1.02238880e-01,  2.06064608e-01, -3.58251050e-01,\n",
       "          2.69169361e-01, -5.74090590e-01,  2.43345177e-01,\n",
       "          1.00872813e-01,  2.33170672e-01, -1.46480116e-02,\n",
       "          9.55578246e-02,  3.12049556e-01, -3.08763418e-01,\n",
       "         -2.24705996e-01, -1.09976520e-03,  2.84712848e-02,\n",
       "         -6.80007417e-01, -4.30494023e-01, -6.37396594e-01,\n",
       "         -1.56302258e-01, -2.46852535e-02,  2.29197141e-01,\n",
       "          1.83732438e-01, -2.24277637e-02, -8.74617488e-02,\n",
       "          3.49416494e-01,  1.88087244e-01,  1.26581763e-01,\n",
       "         -5.56776521e-01,  3.56222549e-01,  3.24567536e-01,\n",
       "         -2.27710746e-01, -3.56966539e-01, -3.99817196e-02,\n",
       "          7.05770728e-01, -4.31589011e-01,  5.20874969e-01,\n",
       "          2.61969540e-01, -3.92927703e-01, -1.97407528e-02,\n",
       "         -1.21434426e-01,  6.74707686e-02,  1.40217870e-01,\n",
       "          6.01783116e-02],\n",
       "        [-4.64356149e-01, -3.80925524e-03,  2.93984764e-01,\n",
       "          3.74743460e-02, -1.87661231e-01,  6.60570823e-02,\n",
       "         -2.46252004e-01, -1.12960803e-01, -2.21997546e-02,\n",
       "         -8.20678128e-02, -2.22118594e-01, -5.02224205e-01,\n",
       "         -2.75100205e-01, -1.68294968e-01,  1.37039780e-01,\n",
       "         -1.19366569e-02, -2.86825292e-01, -2.96567733e-01,\n",
       "          4.10225478e-01, -3.74372703e-01, -6.06027092e-01,\n",
       "         -1.99731338e-01, -3.37373981e-01, -2.05426975e-01,\n",
       "          3.04932660e-02, -1.89209136e-01, -4.57946825e-01,\n",
       "          5.50731671e-01, -2.67717366e-01, -3.47867338e-01,\n",
       "          4.93965408e-01,  5.30537924e-03,  2.85274792e-01,\n",
       "         -4.24161537e-02,  3.50824466e-01, -3.33515390e-01,\n",
       "         -1.31053677e-01, -6.98518548e-01,  1.68566835e-01,\n",
       "          1.90400566e-01,  3.71088234e-01,  1.42132694e-01,\n",
       "          1.18940539e-01,  2.57776583e-01,  4.87458363e-01,\n",
       "          2.22052518e-01, -1.08740353e-01, -1.75685769e-01,\n",
       "          7.74794637e-02, -2.68211580e-01,  1.51052807e-01,\n",
       "          1.18370320e-01, -1.53380830e-01, -1.83204166e-01,\n",
       "          3.64390155e-01, -8.48774120e-02, -2.03386349e-01,\n",
       "          1.69397271e-01,  3.04895296e-01,  4.25265436e-01,\n",
       "         -3.84713015e-01, -1.26443409e-01,  4.77226916e-01,\n",
       "          8.17816234e-02],\n",
       "        [ 4.76312941e-01,  2.41441321e-01, -1.39796802e-01,\n",
       "          4.79673477e-02, -2.57510579e-01, -7.45494891e-01,\n",
       "         -1.21487470e-01, -3.36284435e-03, -1.89285729e-03,\n",
       "          2.49184876e-01, -3.29166146e-01,  2.53351289e-01,\n",
       "         -2.75483581e-02,  3.83280616e-03,  6.41091557e-01,\n",
       "          1.28520663e-01,  6.73023821e-02,  3.20975030e-01,\n",
       "         -9.05835332e-02, -1.52602994e-01,  1.88066168e-01,\n",
       "          1.36170792e-01,  3.11461733e-01,  1.70242688e-01,\n",
       "          1.05610654e-01, -1.09467898e-01,  2.30483445e-01,\n",
       "          5.48435882e-01, -3.51182277e-01,  2.43313415e-01,\n",
       "          1.81744663e-01, -2.50344206e-01,  1.48109484e-01,\n",
       "         -3.27875287e-01,  4.74862042e-01,  5.19988134e-01,\n",
       "          8.09512554e-01,  9.35862314e-02, -1.19003755e-02,\n",
       "          3.53479130e-02,  4.92950947e-01, -1.94227946e-01,\n",
       "         -5.96811070e-01,  6.83531688e-02,  2.31294623e-01,\n",
       "          5.04732986e-01, -3.43235655e-01, -2.63872640e-01,\n",
       "          1.25549665e-01,  2.89218289e-01,  6.32102518e-02,\n",
       "          4.28504604e-01,  1.63764598e-01,  8.23922014e-02,\n",
       "          4.25673264e-01, -1.20435539e-01,  2.90494413e-02,\n",
       "          4.67345661e-01,  2.93655942e-01, -3.05591894e-01,\n",
       "          9.80809606e-02,  2.75508461e-01, -1.22931960e-01,\n",
       "         -1.29834487e-01],\n",
       "        [-3.22078197e-02, -1.73350698e-01,  8.08207953e-01,\n",
       "         -4.53465403e-01,  2.61307896e-01,  3.03751005e-01,\n",
       "         -4.91182989e-02, -1.48665076e-01, -1.38198021e-01,\n",
       "          7.63649262e-02,  5.98217517e-02,  7.10750024e-01,\n",
       "          1.25436013e-01,  1.41080353e-01, -2.13403206e-01,\n",
       "          3.28963029e-01, -1.96625833e-01, -4.84134091e-02,\n",
       "         -2.16646813e-01, -2.43714540e-01, -3.15900805e-01,\n",
       "          2.06320864e-02, -2.43255483e-02,  4.48543554e-01,\n",
       "          4.24543475e-02,  3.95979806e-01, -1.56644489e-01,\n",
       "         -9.40729678e-02, -4.16309932e-02, -1.19545923e-01,\n",
       "         -3.55008368e-01,  1.25127216e-01, -1.00673773e-01,\n",
       "         -4.07603953e-01,  2.14848627e-01, -6.28920220e-01,\n",
       "         -4.55231154e-01,  4.12308431e-01,  3.07361296e-03,\n",
       "          3.31937535e-01,  2.52401710e-01, -4.78765003e-01,\n",
       "          5.19929261e-01, -5.07172662e-01, -7.05329457e-02,\n",
       "          2.19527040e-01,  4.20403394e-01, -2.84162830e-02,\n",
       "          5.55730629e-02,  2.21122401e-01,  2.50578148e-01,\n",
       "         -2.47309984e-01, -2.31294990e-01,  2.39723097e-01,\n",
       "         -2.40794626e-01, -3.41067080e-01, -9.49610278e-02,\n",
       "          1.21244573e-01, -5.07487029e-02, -3.46879559e-02,\n",
       "         -3.68780419e-01,  1.95969194e-02, -2.58298743e-01,\n",
       "         -2.59831026e-01],\n",
       "        [ 3.08084079e-01,  1.32303524e-02, -3.09409416e-01,\n",
       "         -1.67111066e-01,  4.06345604e-01, -8.48772009e-02,\n",
       "         -3.18179858e-01,  2.63261453e-01, -3.37210957e-01,\n",
       "          2.28394109e-01,  2.46335326e-02,  1.28861185e-01,\n",
       "         -3.89098350e-01,  3.44909004e-02, -8.70566856e-01,\n",
       "         -3.69845991e-01,  4.21447369e-01,  3.61056511e-01,\n",
       "          6.46672270e-01, -1.96811266e-01, -2.14087669e-02,\n",
       "         -7.13175889e-01, -9.22556625e-02,  1.80659844e-01,\n",
       "         -2.72314978e-01, -1.38010254e-01, -1.05121707e-01,\n",
       "          2.05757940e-02,  3.55885470e-01, -5.65308365e-01,\n",
       "         -5.72045607e-01, -9.92401518e-02,  3.57369639e-01,\n",
       "          6.85153077e-01,  3.55856862e-01, -2.42200190e-01,\n",
       "         -4.65526439e-01,  1.13650537e-01,  3.29724861e-01,\n",
       "         -6.76272840e-02,  9.63391519e-02,  5.78336399e-01,\n",
       "          2.40074795e-01,  2.34524414e-01, -1.20832844e-01,\n",
       "          1.59713689e-01, -1.01830334e-01, -1.02705810e-01,\n",
       "          2.65789385e-01,  6.52354975e-02,  3.47067328e-01,\n",
       "         -1.12385757e-01,  2.46433977e-01,  4.24273160e-01,\n",
       "         -4.05018149e-02, -7.38231524e-02,  1.04206034e-01,\n",
       "         -1.03487644e-01,  3.36325044e-01, -8.95997529e-02,\n",
       "          4.00681251e-01,  5.57052100e-01, -3.93633183e-01,\n",
       "          5.57920838e-02],\n",
       "        [ 7.12561688e-01,  2.16254182e-01,  4.73272168e-01,\n",
       "         -1.23647103e-01, -9.90262447e-02,  1.09406066e-01,\n",
       "         -1.01828296e-01, -2.84738133e-01, -2.93364197e-01,\n",
       "         -2.30957864e-01,  4.16626157e-01,  1.04062069e-01,\n",
       "         -1.09997440e-01, -4.21199218e-01, -1.75868967e-01,\n",
       "          3.79435415e-02,  4.60783442e-01, -3.06563667e-01,\n",
       "          8.95654586e-03,  1.12304123e-01,  1.55036346e-01,\n",
       "         -5.19232561e-01, -3.82725627e-01,  4.26803001e-01,\n",
       "          6.39851245e-02,  1.41248535e-01, -3.30332770e-01,\n",
       "         -5.45747160e-01,  8.93592996e-02,  5.93004879e-02,\n",
       "         -2.05086017e-01,  4.81562097e-01, -2.42753961e-01,\n",
       "         -2.38080156e-01, -2.82604443e-02, -2.87785823e-01,\n",
       "          2.12118368e-01,  1.09325274e-01, -2.36715938e-02,\n",
       "          3.36661203e-01, -1.65576716e-01,  7.23529362e-01,\n",
       "         -1.53928441e-01,  3.61506937e-01, -3.15008865e-01,\n",
       "          6.07872495e-01, -8.00957842e-02, -3.76310438e-01,\n",
       "          2.28276766e-01, -1.84500344e-01,  1.57720991e-01,\n",
       "          7.53971938e-02, -3.85452263e-01, -5.59431716e-01,\n",
       "          3.98055000e-01, -2.51362212e-01, -4.21642467e-01,\n",
       "          1.05654644e-01, -6.07343292e-01, -2.42599983e-01,\n",
       "         -1.45937123e-01,  7.01104893e-02, -6.84838722e-01,\n",
       "         -7.25018968e-01],\n",
       "        [ 4.40595088e-02,  1.93707966e-01,  3.33663188e-01,\n",
       "         -2.89471545e-01,  3.23218084e-01,  6.68349020e-01,\n",
       "          2.32587151e-01, -1.07753172e-01, -2.74067507e-01,\n",
       "         -7.60728316e-02,  3.45068805e-02,  6.87493385e-04,\n",
       "          1.47938757e-01,  9.65228352e-02,  9.87576897e-02,\n",
       "          2.80280779e-01,  9.50729910e-02, -5.58289932e-02,\n",
       "         -1.55389810e-01,  3.39952644e-02,  2.70519921e-01,\n",
       "          2.38293843e-01,  4.81446357e-01,  5.10011162e-01,\n",
       "         -2.65585291e-01,  2.93632139e-01, -3.55313318e-01,\n",
       "          9.00060670e-02, -3.59439755e-02,  6.35416543e-02,\n",
       "          5.18683918e-02, -5.85446608e-01,  7.89861056e-02,\n",
       "          7.51458517e-03, -7.25475451e-02,  5.36115414e-01,\n",
       "         -9.31870712e-02,  3.65228838e-01, -2.79176685e-01,\n",
       "         -1.11331970e-02, -5.04948418e-01,  2.38147856e-01,\n",
       "         -3.23122671e-02,  2.33532074e-01, -2.70690372e-01,\n",
       "         -7.31856601e-02, -1.52197868e-02,  8.14798553e-01,\n",
       "         -2.29425962e-01, -1.97468188e-01,  1.67872417e-01,\n",
       "         -1.00616401e-01, -4.10148035e-02, -2.40820999e-01,\n",
       "          2.37567944e-01, -1.34605176e-02,  3.82862563e-01,\n",
       "         -4.62833754e-01, -3.58909909e-01, -6.39303193e-02,\n",
       "         -6.87160713e-01, -1.05141137e-01,  2.12156999e-01,\n",
       "          3.09662067e-01],\n",
       "        [-1.43519700e-01, -2.12232712e-01, -7.69391988e-01,\n",
       "          1.02340403e+00, -3.82251754e-01,  3.33307372e-01,\n",
       "         -6.04167929e-01,  5.39302063e-01,  2.91847126e-01,\n",
       "         -5.91689651e-02,  2.22559580e-01, -2.13944257e-02,\n",
       "         -1.50827308e-01,  6.13598735e-01,  1.87574571e-01,\n",
       "          5.74757129e-02,  8.36922576e-02, -6.83881143e-02,\n",
       "         -2.10663332e-01, -7.37238076e-02, -4.66285235e-01,\n",
       "         -3.00057880e-01,  4.98590671e-01,  1.35872925e-01,\n",
       "         -2.69460051e-01,  1.11020324e-01,  1.50184520e-01,\n",
       "         -5.70108919e-01, -6.05392049e-02, -9.85916416e-02,\n",
       "          1.55745669e-01, -3.91152295e-01, -3.59703481e-01,\n",
       "         -3.69432724e-01, -4.79959420e-01,  5.68561918e-01,\n",
       "          1.34798597e-01,  6.35782737e-02, -1.32616551e-01,\n",
       "         -6.72587126e-02,  2.35174076e-01,  3.29788049e-01,\n",
       "         -1.02327091e-02,  3.98597490e-01, -5.20400995e-02,\n",
       "          5.18388328e-01,  2.98432342e-01, -3.25397334e-01,\n",
       "         -2.12241578e-01,  4.90125594e-01, -4.08107751e-01,\n",
       "         -1.47850698e-01, -2.50007973e-01,  3.31194252e-01,\n",
       "          9.19282450e-01,  5.39550956e-01, -1.75560938e-01,\n",
       "         -5.08892770e-02,  5.93267496e-01,  7.91356800e-02,\n",
       "          1.31953898e-01, -1.41951275e-03,  2.23207980e-01,\n",
       "         -5.46580448e-02]]),\n",
       " 'A0': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32941176, 0.99607843, 0.39607843,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.68235294, 0.99215686, 0.46666667, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.12156863, 0.96862745, 0.79215686,\n",
       "        0.11372549, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00392157, 0.00392157,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55294118, 0.99215686, 0.65882353, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25882353, 0.81568627, 0.21960784, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.72941176, 0.99215686,\n",
       "        0.47058824, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22352941,\n",
       "        0.99215686, 0.46666667, 0.        , 0.        , 0.        ,\n",
       "        0.10980392, 0.97647059, 0.94117647, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13333333, 0.99215686, 0.46666667,\n",
       "        0.        , 0.        , 0.        , 0.42745098, 0.99607843,\n",
       "        0.77254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20784314, 0.99215686, 0.46666667, 0.        , 0.        ,\n",
       "        0.        , 0.52941176, 0.99607843, 0.52156863, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.52156863, 0.99607843,\n",
       "        0.46666667, 0.        , 0.        , 0.10588235, 0.94117647,\n",
       "        1.        , 0.1372549 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02745098, 0.92156863, 0.99215686, 0.81568627, 0.59215686,\n",
       "        0.6627451 , 0.84313725, 0.99215686, 0.80784314, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.38039216, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99607843, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.3372549 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.58823529, 0.95686275, 0.56862745, 0.46666667,\n",
       "        0.39607843, 0.32156863, 0.99215686, 0.99215686, 0.05490196,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.32941176,\n",
       "        0.99607843, 0.6745098 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.68235294, 0.99215686, 0.46666667,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.92941176, 0.98823529, 0.21960784, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19607843, 0.94509804, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.73333333, 0.99607843, 0.97647059, 0.41176471, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.72941176, 0.99215686,\n",
       "        0.80784314, 0.08235294, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.89019608, 0.94901961, 0.1254902 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.9254902 ,\n",
       "        0.85882353, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'Z1': array([ 0.92105261, -1.08297302,  1.67874031,  0.54509699,  0.87543218,\n",
       "        -0.67008979,  0.41703138,  0.37834007, -0.1616296 ,  0.19345973,\n",
       "         0.0537064 , -0.54797285,  0.5291328 , -0.29775919,  1.00622006,\n",
       "        -0.16691313, -0.26708018,  0.65706965, -0.55723975, -1.25244682,\n",
       "         0.41887611,  0.77695509,  0.02447041, -1.16247795,  0.08834548,\n",
       "         0.16410556, -0.40563608,  0.12280436,  0.78473587, -0.59000206,\n",
       "        -0.00314717, -0.40181294,  0.21437546, -0.16461458, -0.18253434,\n",
       "         0.4281059 ,  0.22686371,  0.10230054,  0.71135309,  0.46260937,\n",
       "        -0.37498054,  0.58033548, -0.53833834, -0.3391189 ,  0.82334774,\n",
       "        -0.54769315, -0.3067636 ,  1.04712266, -0.63283012,  0.38389253,\n",
       "        -0.35120267,  0.01055482, -0.56687731,  0.43943406, -1.47706142,\n",
       "        -1.40839029, -1.09664188, -0.3591571 , -0.93465028, -0.51986062,\n",
       "         0.15901136,  0.507775  ,  0.24394084, -1.28579343, -0.06765948,\n",
       "        -0.02083227,  0.14025643, -0.72586605, -0.14049343,  0.03494707,\n",
       "        -1.44685778,  0.63662185, -0.65574794, -0.06861761, -1.02557144,\n",
       "         0.37552519, -1.16867662, -0.22408655, -0.32542492,  0.32577652,\n",
       "         0.09595565,  0.32217408, -0.27801723, -1.91737428, -0.98874157,\n",
       "         0.37190569,  0.87772377,  0.03631852,  0.43526791, -0.61232257,\n",
       "        -0.0210663 , -0.54268792, -0.61285968,  0.3081252 ,  0.28841052,\n",
       "         1.0901487 ,  1.06231499, -0.90593896, -0.40843688, -0.58926909,\n",
       "        -0.12978984, -0.98732455, -1.80659672, -0.48877359,  1.17422206,\n",
       "        -1.01590686,  0.71515924, -0.2270138 ,  0.28676989,  0.78781667,\n",
       "         0.17273048, -0.9690459 ,  0.2212246 , -0.15444274,  0.38102285,\n",
       "         0.11133317, -0.56956566, -1.10998352,  0.7270916 , -0.21318442,\n",
       "         1.39037952, -0.12616752, -0.83631533, -0.25831686,  0.29280853,\n",
       "         0.12961392, -1.07301105, -0.25392243]),\n",
       " 'A1': array([0.92105261, 0.        , 1.67874031, 0.54509699, 0.87543218,\n",
       "        0.        , 0.41703138, 0.37834007, 0.        , 0.19345973,\n",
       "        0.0537064 , 0.        , 0.5291328 , 0.        , 1.00622006,\n",
       "        0.        , 0.        , 0.65706965, 0.        , 0.        ,\n",
       "        0.41887611, 0.77695509, 0.02447041, 0.        , 0.08834548,\n",
       "        0.16410556, 0.        , 0.12280436, 0.78473587, 0.        ,\n",
       "        0.        , 0.        , 0.21437546, 0.        , 0.        ,\n",
       "        0.4281059 , 0.22686371, 0.10230054, 0.71135309, 0.46260937,\n",
       "        0.        , 0.58033548, 0.        , 0.        , 0.82334774,\n",
       "        0.        , 0.        , 1.04712266, 0.        , 0.38389253,\n",
       "        0.        , 0.01055482, 0.        , 0.43943406, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.15901136, 0.507775  , 0.24394084, 0.        , 0.        ,\n",
       "        0.        , 0.14025643, 0.        , 0.        , 0.03494707,\n",
       "        0.        , 0.63662185, 0.        , 0.        , 0.        ,\n",
       "        0.37552519, 0.        , 0.        , 0.        , 0.32577652,\n",
       "        0.09595565, 0.32217408, 0.        , 0.        , 0.        ,\n",
       "        0.37190569, 0.87772377, 0.03631852, 0.43526791, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3081252 , 0.28841052,\n",
       "        1.0901487 , 1.06231499, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.17422206,\n",
       "        0.        , 0.71515924, 0.        , 0.28676989, 0.78781667,\n",
       "        0.17273048, 0.        , 0.2212246 , 0.        , 0.38102285,\n",
       "        0.11133317, 0.        , 0.        , 0.7270916 , 0.        ,\n",
       "        1.39037952, 0.        , 0.        , 0.        , 0.29280853,\n",
       "        0.12961392, 0.        , 0.        ]),\n",
       " 'Z2': array([ 0.4571827 , -0.37403641, -0.34867284,  0.46523749, -0.54077635,\n",
       "         0.3294311 , -0.3412037 , -0.77467876,  0.16707371, -1.21963122,\n",
       "        -0.24901384,  0.03412443, -0.2180674 ,  0.83973685,  0.29289794,\n",
       "         0.26215775,  0.15001483,  0.28528552, -0.23199489, -0.53992868,\n",
       "         0.32272733,  0.64001401, -0.14100816,  0.94557214,  0.35033439,\n",
       "         1.30066592, -0.07689283, -0.88795271, -0.54605143,  1.35354901,\n",
       "         0.30011487, -0.50452544,  0.21736054,  0.78648723,  0.06105454,\n",
       "         0.95196085,  0.25774742,  0.86252986,  0.5428027 ,  1.13552398,\n",
       "         1.20116663,  0.070538  ,  0.79896934,  0.08558201,  0.09130741,\n",
       "         0.43530334,  0.66385924, -0.09993068,  0.57757868, -0.14610443,\n",
       "        -0.1048357 , -0.06747871,  0.15584671, -0.7786281 , -0.01300121,\n",
       "         0.34361091,  0.65734796, -0.1139023 , -0.34050337, -0.16179612,\n",
       "         0.26701778,  0.005237  ,  0.76060028,  0.00293416]),\n",
       " 'A2': array([0.4571827 , 0.        , 0.        , 0.46523749, 0.        ,\n",
       "        0.3294311 , 0.        , 0.        , 0.16707371, 0.        ,\n",
       "        0.        , 0.03412443, 0.        , 0.83973685, 0.29289794,\n",
       "        0.26215775, 0.15001483, 0.28528552, 0.        , 0.        ,\n",
       "        0.32272733, 0.64001401, 0.        , 0.94557214, 0.35033439,\n",
       "        1.30066592, 0.        , 0.        , 0.        , 1.35354901,\n",
       "        0.30011487, 0.        , 0.21736054, 0.78648723, 0.06105454,\n",
       "        0.95196085, 0.25774742, 0.86252986, 0.5428027 , 1.13552398,\n",
       "        1.20116663, 0.070538  , 0.79896934, 0.08558201, 0.09130741,\n",
       "        0.43530334, 0.66385924, 0.        , 0.57757868, 0.        ,\n",
       "        0.        , 0.        , 0.15584671, 0.        , 0.        ,\n",
       "        0.34361091, 0.65734796, 0.        , 0.        , 0.        ,\n",
       "        0.26701778, 0.005237  , 0.76060028, 0.00293416]),\n",
       " 'Z3': array([-2.58822681,  1.00806315, -2.1917368 , -1.33791993,  1.66776573,\n",
       "         0.79067034, -0.78489728, -0.71651693,  1.43416772,  1.84431674]),\n",
       " 'A3': array([0.0033915 , 0.1236632 , 0.00504179, 0.01184113, 0.23919146,\n",
       "        0.09950112, 0.0205858 , 0.0220427 , 0.1893631 , 0.2853782 ])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.params['W1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = 784\n",
    "hidden_1 = 128\n",
    "hidden_2 = 64\n",
    "output_layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "    'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "    'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer activations becomes sample\n",
    "params['A0'] = x_train[0]\n",
    "\n",
    "# input layer to hidden layer 1\n",
    "params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "params['A1'] = dnn.activation(params['Z1'])\n",
    "\n",
    "# hidden layer 1 to hidden layer 2\n",
    "params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "params['A2'] = dnn.activation(params['Z2'])\n",
    "\n",
    "# hidden layer 2 to output layer\n",
    "params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "params['A3'] = dnn.softmax(params['Z3'])\n",
    "\n",
    "output = params['A3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_w = {}\n",
    "\n",
    "# Calculate W3 update\n",
    "error = output - y_train[0]\n",
    "change_w['W3'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A2'], (params['A2'].shape[0], 1)).T)\n",
    "#         change_w['W3'] = np.dot(error, params['A3'])\n",
    "\n",
    "# Calculate W2 update\n",
    "error = np.multiply(np.dot(params['W3'].T, error), dnn.activation(params['Z2'], derivative=True) )\n",
    "change_w['W2'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A1'], (params['A1'].shape[0], 1)).T)\n",
    "\n",
    "# Calculate W1 update\n",
    "error = np.multiply(np.dot(params['W2'].T, error), dnn.activation(params['Z1'], derivative=True) )\n",
    "change_w['W1'] = np.dot(np.reshape(error, (error.shape[0], 1)), np.reshape(params['A0'], (params['A0'].shape[0], 1)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.multiply(np.dot(params['W3'].T, error), dnn.activation(params['Z2'], derivative=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_w['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,) and (64,) not aligned: 10 (dim 0) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ff19f019159b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3d358801b5a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, x_val, y_val, batch_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mchanges_to_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mchanges_to_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3d358801b5a8>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, y_train, output)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Calculate W3 update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mchange_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;31m#         change_w['W3'] = np.dot(error, params['A3'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,) and (64,) not aligned: 10 (dim 0) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10], activation='relu')\n",
    "change = dnn.train(x_train, y_train, x_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 18.77s, Accuracy: 79.211\n",
      "Epoch: 2, Time Spent: 33.36s, Accuracy: 87.23100000000001\n",
      "Epoch: 3, Time Spent: 49.17s, Accuracy: 81.49199999999999\n",
      "Epoch: 4, Time Spent: 66.17s, Accuracy: 90.0\n",
      "Epoch: 5, Time Spent: 79.97s, Accuracy: 90.0\n",
      "Epoch: 6, Time Spent: 93.46s, Accuracy: 90.0\n",
      "Epoch: 7, Time Spent: 105.89s, Accuracy: 90.0\n",
      "Epoch: 8, Time Spent: 120.99s, Accuracy: 90.0\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good exercises in NumPy\n",
    "You might have noticed that the code is very readable, but takes up a lot of space and could be optimized to run in loops. Here is a chance to optimize and improve the code. For example, you can optimize the forward and backward pass, such that they run in a for loop in each function. This makes the code easier to modify and possibly easier to maintain. \n",
    "\n",
    "More challenging exercises including implement any other activation function from this overview of activation functions, and remember to implement the derivatives as well. Different optimizers, e.g. Adam, RMSProp, etc, and different loss functions, e.g. MSE, cross-entropy loss are also worth to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
