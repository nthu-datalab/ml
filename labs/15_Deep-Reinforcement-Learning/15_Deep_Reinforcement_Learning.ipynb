{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArTw-MslqlO8"
   },
   "source": [
    "# <center>Deep Reinforcement Learning </center>\n",
    "<center>Shan-Hung Wu & DataLab</center>\n",
    "<center>Fall 2023</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIuBQdg3qlO-"
   },
   "source": [
    "In the last lab, we use the tabular method (Q-learning, SARSA) to train an agent to play *Flappy Bird* with features in environments. However, it is time-costly and inefficient if more features are added to the environment because the agent can not easily generalize its experience to other states that were not seen before. Furthermore, in realistic environments with large state / action space, it requires a large memory space to store all state-action pairs.  \n",
    "In this lab, we introduce deep reinforcement learning, which utilizes function approximation to estimate value / policy for all unseen states such that given a state, we can estimate its value or action. We can use what we have learned in machine learning (e.g. regression, DNN) to achieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u5rDY_gqlO-"
   },
   "source": [
    "# PPO X GAE\n",
    "*Reference*: [Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438.pdf), [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "\n",
    "To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations.  \n",
    "In this lab, we are going to train an agent which takes raw frames as input instead of hand-crafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jln2-NPjqBiJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy.editor as mpy\n",
    "import skimage.transform\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT2phgNLqBiL"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\") \n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxeQV3UqqBiM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()\n",
    "\n",
    "test_game = FlappyBird()\n",
    "test_env = PLE(test_game, fps=30, display_screen=False)\n",
    "test_env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './movie_f' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2zdfMwWqBiM"
   },
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'image_size': 84,\n",
    "    'num_stack': 4,\n",
    "    'action_dim': len(env.getActionSet()),\n",
    "    'hidden_size': 256,\n",
    "    'lr': 0.0001,\n",
    "    'gamma': 0.99,\n",
    "    'lambda': 0.95,\n",
    "    'clip_val': 0.2,\n",
    "    'ppo_epochs': 8,\n",
    "    'test_epochs': 1,\n",
    "    'num_steps': 512,\n",
    "    'mini_batch_size': 64,\n",
    "    'target_reward': 200,\n",
    "    'max_episode': 30000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps_CfrFdqBiN"
   },
   "outputs": [],
   "source": [
    "# Please do not modify this method\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    \n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enTlOuPsqBiN"
   },
   "outputs": [],
   "source": [
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.rotate(screen, -90, resize=True)\n",
    "    screen = screen[:400, :]\n",
    "    screen = skimage.transform.resize(screen, [hparas['image_size'], hparas['image_size'], 1])\n",
    "    return screen.astype(np.float32)\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if(len(input_frames) == 1):\n",
    "        state = np.concatenate(input_frames*4, axis=-1)\n",
    "    elif(len(input_frames) == 2):\n",
    "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "    elif(len(input_frames) == 3):\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9rMFN1VqBiN"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = tf.keras.Sequential([\n",
    "          # Convolutional Layers\n",
    "          tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          # Embedding Layers\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(hparas['hidden_size']),\n",
    "          tf.keras.layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = tf.keras.layers.Dense(hparas['action_dim'], activation='softmax')\n",
    "        # Critic Network\n",
    "        self.critic = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.feature_extractor(input)\n",
    "        action_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C564HfI4qBiO"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hparas):\n",
    "        self.gamma = hparas['gamma']\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=hparas['lr'])\n",
    "        self.actor_critic = ActorCriticNetwork(hparas)\n",
    "        self.clip_pram = hparas['clip_val']\n",
    "    \n",
    "    def ppo_iter(self, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "        batch_size = states.shape[0]\n",
    "        for _ in range(batch_size // mini_batch_size):\n",
    "            rand_ids = tf.convert_to_tensor(np.random.randint(0, batch_size, mini_batch_size), dtype=tf.int32)\n",
    "            yield tf.gather(states, rand_ids), tf.gather(actions, rand_ids), tf.gather(log_probs, rand_ids), \\\n",
    "             tf.gather(returns, rand_ids), tf.gather(advantage, rand_ids)\n",
    "    \n",
    "    def ppo_update(self, ppo_epochs, mini_batch_size, states, actions, log_probs, discount_rewards, advantages):       \n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        for _ in range(ppo_epochs):\n",
    "            for state, action, old_log_probs, reward, advantage in self.ppo_iter(mini_batch_size, states, actions, log_probs, discount_rewards, advantages):\n",
    "                reward = tf.expand_dims(reward, axis=-1)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prob, value = self.actor_critic(state, training=True)\n",
    "                    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "                    entropy = tf.math.reduce_mean(dist.entropy())\n",
    "                    new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                    # PPO ratio\n",
    "                    ratio = tf.math.exp(new_log_probs - old_log_probs)\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram) * advantage\n",
    "\n",
    "                    actor_loss = tf.math.negative(tf.math.reduce_mean(tf.math.minimum(surr1, surr2))) - 0.1 * entropy\n",
    "                    critic_loss = 0.5 * tf.math.reduce_mean(kls.mean_squared_error(reward, value))\n",
    "\n",
    "                    total_loss = actor_loss + critic_loss\n",
    "            \n",
    "                # single optimizer\n",
    "                grads = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.actor_critic.trainable_variables))\n",
    "      \n",
    "                total_actor_loss += actor_loss\n",
    "                total_critic_loss += critic_loss\n",
    "        return total_actor_loss, total_critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5TmWPGzqBiO"
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1506.02438.pdf\n",
    "# Equation 16\n",
    "def compute_gae(rewards, masks, values, gamma, LAMBDA):\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * LAMBDA * masks[i] * gae\n",
    "        returns.append(gae + values[i])\n",
    "\n",
    "    returns.reverse()\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT-jjpKkqBiP"
   },
   "source": [
    "## Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxodxa0dqBiP"
   },
   "outputs": [],
   "source": [
    "def test_reward(test_env, agent):\n",
    "    total_reward = 0\n",
    "    # Reset the environment\n",
    "    test_env.reset_game()\n",
    "    input_frames = [preprocess_screen(test_env.getScreenGrayscale())]\n",
    "\n",
    "    while not test_env.game_over():\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        action = np.argmax(prob[0].numpy())\n",
    "        reward = test_env.act(test_env.getActionSet()[action])\n",
    "        total_reward += reward\n",
    "\n",
    "        input_frames.append(preprocess_screen(test_env.getScreenGrayscale()))\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70xtqCpSqBiP"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv2obCXwqBiQ"
   },
   "outputs": [],
   "source": [
    "agent = Agent(hparas)\n",
    "max_episode = hparas['max_episode']\n",
    "test_per_n_episode = 10\n",
    "force_save_per_n_episode = 1000\n",
    "early_stop_reward = 10\n",
    "\n",
    "start_s = 0\n",
    "best_reward = -5.0\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    actor_critic = agent.actor_critic,\n",
    "    optimizer = agent.optimizer,\n",
    ")\n",
    "\n",
    "# Load from old checkpoint\n",
    "# checkpoint.restore('ckpt_dir/ckpt-?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09HTUD4HqBiQ"
   },
   "outputs": [],
   "source": [
    "ep_reward = []\n",
    "total_avgr = []\n",
    "early_stop = False\n",
    "avg_rewards_list = []\n",
    "\n",
    "env.reset_game()\n",
    "\n",
    "for s in range(0, max_episode):\n",
    "    if early_stop == True:\n",
    "        break\n",
    "\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    masks = []\n",
    "    values = []\n",
    "\n",
    "    display_frames = [env.getScreenRGB()]\n",
    "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "\n",
    "    for step in range(hparas['num_steps']):\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        dist = tfp.distributions.Categorical(probs=prob[0], dtype=tf.float32)\n",
    "        action = dist.sample(1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        reward = env.act(env.getActionSet()[int(action.numpy())])\n",
    "\n",
    "        done = env.game_over()\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        values.append(value[0])\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(tf.convert_to_tensor(reward, dtype=tf.float32))\n",
    "        masks.append(tf.convert_to_tensor(1-int(done), dtype=tf.float32))\n",
    "\n",
    "        display_frames.append(env.getScreenRGB())\n",
    "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "\n",
    "        if done:\n",
    "            env.reset_game()\n",
    "            input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "  \n",
    "    _, next_value = agent.actor_critic(state)\n",
    "    values.append(next_value[0])\n",
    "\n",
    "    returns = compute_gae(rewards, masks, values, hparas['gamma'], hparas['lambda'])\n",
    "\n",
    "    returns = tf.concat(returns, axis=0)\n",
    "    log_probs = tf.concat(log_probs, axis=0)\n",
    "    values = tf.concat(values, axis=0)\n",
    "    states = tf.concat(states, axis=0)\n",
    "    actions = tf.concat(actions, axis=0)\n",
    "    advantage = returns - values[:-1]\n",
    "\n",
    "    a_loss, c_loss = agent.ppo_update(hparas['ppo_epochs'], hparas['mini_batch_size'], states, actions, log_probs, returns, advantage)\n",
    "    print('[Episode %d]  Actor loss: %.5f, Critic loss: %.5f' % (s, a_loss, c_loss))\n",
    "\n",
    "    if s % test_per_n_episode == 0:\n",
    "        # test agent hparas['test_epochs'] times to get the average reward\n",
    "        avg_reward = np.mean([test_reward(test_env, agent) for _ in range(hparas['test_epochs'])])\n",
    "        print(\"Test average reward is %.1f, Current best average reward is %.1f\\n\" % (avg_reward, best_reward))\n",
    "        avg_rewards_list.append(avg_reward)\n",
    "\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            agent.actor_critic.save('./save/Actor/model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "            checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
    "\n",
    "    if s % force_save_per_n_episode == 0:\n",
    "        agent.actor_critic.save('./save/Actor/model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        checkpoint.save(file_prefix = './save/checkpoints/ckpt')\n",
    "        clip = make_anim(display_frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie_f/{}_demo-{}.webm\".format('Lab15', s), fps=60)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))\n",
    "\n",
    "    if best_reward >= early_stop_reward:\n",
    "        early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poRJe43BqlPE"
   },
   "source": [
    "# <center>Assignment</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUvQcrMeqlPE"
   },
   "source": [
    "## What you should do:\n",
    "- Run the code and comprehense it \n",
    "- Write your discovery in this notebook (exempli gratia how many times your birds fly to get more than 10 rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKIHwg7pqlPE"
   },
   "source": [
    "## Evaluation metrics:\n",
    "- Report of this lab (50%)\n",
    "- The bird is able to fly through at least 1 pipe (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI4hN4TxqlPE"
   },
   "source": [
    "## Requirements:\n",
    "- Upload the notebook to eeclass\n",
    "    - Lab15_{student_id}.ipynb\n",
    "- **Deadline: 2024-01-04 (Thur) 23:59**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "17_Deep-Reinforcement-Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
